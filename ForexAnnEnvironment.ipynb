{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ForexAnnEnvironment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1Y23CLo2oUco-O4CAkKZhYsjzzzbnTeLP",
      "authorship_tag": "ABX9TyOqnoCC4RSCbYwqjnZVEPo8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robLaing2/Forex_ANN_Forecasting/blob/master/ForexAnnEnvironment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0f_HWfsWuF",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIPPZ27ksNag",
        "colab_type": "code",
        "outputId": "aa41fe39-53f3-4229-80a8-eb3bab828656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!pip install quandl\n",
        "!pip install dbnomics\n",
        "#!pip install FRB\n",
        "!pip install fred\n",
        "!pip install mock\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow==2.0.0\n",
        "\n",
        "import fred\n",
        "from mock import Mock\n",
        "import requests\n",
        "import json\n",
        "import quandl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from dbnomics import fetch_series\n",
        "import pandas as pd\n",
        "from keras.models import model_from_json\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "START_DATE = '2016-01-01'\n",
        "END_DATE = ''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Collecting quandl\n",
            "  Downloading https://files.pythonhosted.org/packages/07/ab/8cd479fba8a9b197a43a0d55dd534b066fb8e5a0a04b5c0384cbc5d663aa/Quandl-3.5.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from quandl) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /tensorflow-2.1.0/python3.6 (from quandl) (1.18.1)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.25.3)\n",
            "Requirement already satisfied: requests>=2.7.0 in /tensorflow-2.1.0/python3.6 (from quandl) (2.23.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl) (8.2.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from quandl) (2.6.1)\n",
            "Collecting inflection>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/35/a6eb45b4e2356fe688b21570864d4aa0d0a880ce387defe9c589112077f8/inflection-0.3.1.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.7.0->quandl) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.7.0->quandl) (1.25.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.7.0->quandl) (2.9)\n",
            "Building wheels for collected packages: inflection\n",
            "  Building wheel for inflection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for inflection: filename=inflection-0.3.1-cp36-none-any.whl size=6077 sha256=f89a5b76752e7497c03a97409fd89d3e1f858aca626fd0d8b01f1c39964de08e\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/5a/d3/6fc3bf6516d2a3eb7e18f9f28b472110b59325f3f258fe9211\n",
            "Successfully built inflection\n",
            "Installing collected packages: inflection, quandl\n",
            "Successfully installed inflection-0.3.1 quandl-3.5.0\n",
            "Collecting dbnomics\n",
            "  Downloading https://files.pythonhosted.org/packages/40/61/ea2768574fe423708777b01bd054a23cb40eb25e32f68b49f485b99ff0b0/DBnomics-1.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.6/dist-packages (from dbnomics) (0.25.3)\n",
            "Requirement already satisfied: requests>=2.18.4 in /tensorflow-2.1.0/python3.6 (from dbnomics) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->dbnomics) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->dbnomics) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /tensorflow-2.1.0/python3.6 (from pandas>=0.21->dbnomics) (1.18.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.18.4->dbnomics) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.18.4->dbnomics) (1.25.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.18.4->dbnomics) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.18.4->dbnomics) (3.0.4)\n",
            "Requirement already satisfied: six>=1.5 in /tensorflow-2.1.0/python3.6 (from python-dateutil>=2.6.1->pandas>=0.21->dbnomics) (1.14.0)\n",
            "Installing collected packages: dbnomics\n",
            "Successfully installed dbnomics-1.2.0\n",
            "Collecting fred\n",
            "  Downloading https://files.pythonhosted.org/packages/30/56/6c3ad3a271dfe2703da1e127e3f0beafd98742bf84b756773521518b5eaa/fred-3.1.tar.gz\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from fred) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->fred) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->fred) (1.25.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->fred) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->fred) (2019.11.28)\n",
            "Building wheels for collected packages: fred\n",
            "  Building wheel for fred (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fred: filename=fred-3.1-cp36-none-any.whl size=3911 sha256=9e41ece6e4df1eed5ded2dd05cd56157d54371e4b4b053c1968eb2aa1717f156\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/4c/87/eb32bd051f3888ff21dbcd0cbc8a5512579aba99213e7d3183\n",
            "Successfully built fred\n",
            "Installing collected packages: fred\n",
            "Successfully installed fred-3.1\n",
            "Collecting mock\n",
            "  Downloading https://files.pythonhosted.org/packages/30/6a/9bde648117ec7087c89a45de0a8b25aba21d54d3defd08cb24eacded875f/mock-4.0.1-py3-none-any.whl\n",
            "Installing collected packages: mock\n",
            "Successfully installed mock-4.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knovwSza04MP",
        "colab_type": "text"
      },
      "source": [
        "# Data Prepartion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrnOCJajGrND",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing Interest Rate data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBT7oI__I2MS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GBPovr = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBPONTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE)\n",
        "EURovr = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EURONTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE)\n",
        "GBP1month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP1MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE)\n",
        "EUR1month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR1MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE)\n",
        "GBP3month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP3MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE)\n",
        "EUR3month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR3MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE)\n",
        "GBP6month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP6MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE)\n",
        "EUR6month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR6MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE)\n",
        "GBP12month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP12MD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE)\n",
        "EUR12month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR12MD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE)\n",
        "\n",
        "GBRovrJson = (json.loads(GBPovr.content))[\"observations\"]\n",
        "EURovrJson = (json.loads(EURovr.content))[\"observations\"]\n",
        "GBR1mJson = (json.loads(GBP1month.content))[\"observations\"]\n",
        "EUR1mJson = (json.loads(EUR1month.content))[\"observations\"]\n",
        "GBR3mJson = (json.loads(GBP3month.content))[\"observations\"]\n",
        "EUR3mJson = (json.loads(EUR3month.content))[\"observations\"]\n",
        "GBR6mJson = (json.loads(GBP6month.content))[\"observations\"]\n",
        "EUR6mJson = (json.loads(EUR6month.content))[\"observations\"]\n",
        "GBR12mJson = (json.loads(GBP12month.content))[\"observations\"]\n",
        "EUR12mJson = (json.loads(EUR12month.content))[\"observations\"]\n",
        "\n",
        "def getRatioValues(datasetX, datasetY):\n",
        "    dates = []\n",
        "    valuesX = []\n",
        "    valuesY = []\n",
        "\n",
        "    for x in range(len(datasetX)):\n",
        "        if (datasetX[x][\"value\"] == '.' or datasetY[x][\"value\"] == '.'):\n",
        "            continue\n",
        "\n",
        "        dates.append(pd.Timestamp(datasetX[x][\"date\"]))\n",
        "        valuesX.append(float(datasetX[x][\"value\"]))\n",
        "        valuesY.append(float(datasetY[x][\"value\"]))\n",
        "\n",
        "    datasetXarr = np.array(valuesX, dtype=np.float)\n",
        "    datasetYarr = np.array(valuesY, dtype=np.float)\n",
        "\n",
        "    ratioValues = datasetXarr / datasetYarr\n",
        "\n",
        "    data_mean = ratioValues.mean()\n",
        "    data_std = ratioValues.std()\n",
        "    dataNormalised = (ratioValues - data_mean) / data_std\n",
        "\n",
        "    res = {dates[i]: dataNormalised[i] for i in range(len(dates))}\n",
        "\n",
        "    return res\n",
        "\n",
        "GBPEURovrRatio = getRatioValues(GBRovrJson,EURovrJson)\n",
        "\n",
        "GBPEUR1mRatio = getRatioValues(GBR1mJson,EUR1mJson)\n",
        "GBPEUR3mRatio = getRatioValues(GBR3mJson,EUR3mJson)\n",
        "GBPEUR6mRatio = getRatioValues(GBR6mJson,EUR6mJson)\n",
        "GBPEUR12mRatio = getRatioValues(GBR12mJson,EUR12mJson)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4pR6Orq9ma",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing Inflation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBLa-rIMNPut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ukCPI = fetch_series('IMF/CPI/M.GB.PCPIHA_PC_CP_A_PT')\n",
        "euCPI = fetch_series('IMF/CPI/M.U2.PCPIHA_PC_CP_A_PT')\n",
        "\n",
        "dbnomicsQuery = \"period >= '\" + START_DATE + \"'\"\n",
        "\n",
        "# and period < '2019-06-01'\"\n",
        "\n",
        "ukCPI = ukCPI.query(dbnomicsQuery)\n",
        "euCPI = euCPI.query(dbnomicsQuery)\n",
        "\n",
        "ukCPIdates = []\n",
        "for x in ukCPI.period:\n",
        "    ukCPIdates.append(x)\n",
        "\n",
        "ukCPIarr = np.array(ukCPI.value, dtype=np.float)\n",
        "euCPIarr = np.array(euCPI.value, dtype=np.float)\n",
        "\n",
        "ukEuCpiRatio = ukCPIarr*1000 / euCPIarr*1000\n",
        "\n",
        "# Normalise CPI data\n",
        "cpi_mean = ukEuCpiRatio.mean()\n",
        "cpi_std = ukEuCpiRatio.std()\n",
        "ukEuCpiRatio = (ukEuCpiRatio - cpi_mean) / cpi_std\n",
        "\n",
        "cpiDict = {ukCPIdates[i]: ukEuCpiRatio[i] for i in range(len(ukCPIdates))}\n",
        "\n",
        "cpiData = {'Date':ukCPI.period, 'Value':ukEuCpiRatio}\n",
        "cpiDf = pd.DataFrame(cpiData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz9f5MPUOBva",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing International Reserves data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu_mv-VUOMyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ukIR = fetch_series('IMF/IFS/M.GB.RAFAGOLDM_USD')\n",
        "euIR = fetch_series('IMF/IFS/M.U2.RAFAGOLDM_USD')\n",
        "\n",
        "dbnomicsQuery = \"period >= '\" + START_DATE + \"'\"\n",
        "# and period < '2019-06-01'\"\n",
        "\n",
        "ukIR = ukIR.query(dbnomicsQuery)\n",
        "euIR = euIR.query(dbnomicsQuery)\n",
        "\n",
        "ukIRdates = []\n",
        "for x in ukIR.period:\n",
        "    ukIRdates.append(x)\n",
        "\n",
        "ukIRarr = np.array(ukIR.value, dtype=np.float)\n",
        "euIRarr = np.array(euIR.value, dtype=np.float)\n",
        "\n",
        "ukEuIRRatio = ukIRarr*1000 / euIRarr*1000\n",
        "\n",
        "ir_mean = ukEuIRRatio.mean()\n",
        "ir_std = ukEuIRRatio.std()\n",
        "ukEuIRRatio = (ukEuIRRatio - ir_mean) / ir_std\n",
        "\n",
        "irDict = {ukIRdates[i]: ukEuIRRatio[i] for i in range(len(ukIRdates))}\n",
        "\n",
        "irData = {'Date':ukIR.period, 'Value':ukEuIRRatio}\n",
        "irDf = pd.DataFrame(irData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVW8UDb5OziA",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing Balance of Payments data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYGfT2feO9Yb",
        "colab_type": "code",
        "outputId": "cf273d30-34f4-4984-97f8-267808f63fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "ukBOP = fetch_series('IMF/BOP/Q.GB.BACK_BP6_USD')\n",
        "euBOP = fetch_series('IMF/BOP/Q.U2.BACK_BP6_USD')\n",
        "\n",
        "dbnomicsQuery = \"period >= '\" + START_DATE + \"'\"\n",
        "# and period < '2019-06-01'\"\n",
        "\n",
        "ukBOP = ukBOP.query(dbnomicsQuery)\n",
        "euBOP = euBOP.query(dbnomicsQuery)\n",
        "\n",
        "ukBOPdates = []\n",
        "for x in ukBOP.period:\n",
        "    ukBOPdates.append(x)\n",
        "\n",
        "ukBOParr = np.array(ukBOP.value, dtype=np.float)\n",
        "euBOParr = np.array(euBOP.value, dtype=np.float)\n",
        "\n",
        "ukEuBOPRatio = ukBOParr*1000 / euBOParr*1000\n",
        "\n",
        "# Normalise BOP data\n",
        "bop_mean = ukEuBOPRatio.mean()\n",
        "bop_std = ukEuBOPRatio.std()\n",
        "ukEuBOPRatio = (ukEuBOPRatio - bop_mean) / bop_std\n",
        "\n",
        "bopDict = {ukBOPdates[i]: ukEuBOPRatio[i] for i in range(len(ukBOPdates))}\n",
        "\n",
        "bopData = {'Date':ukBOP.period, 'Value':ukEuBOPRatio}\n",
        "bopDf = pd.DataFrame(bopData)\n",
        "\n",
        "print(bopDict)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{Timestamp('2016-01-01 00:00:00'): -0.6259830251612936, Timestamp('2016-04-01 00:00:00'): 0.2568904128584844, Timestamp('2016-07-01 00:00:00'): -0.01006309969593224, Timestamp('2016-10-01 00:00:00'): 0.8881103288351604, Timestamp('2017-01-01 00:00:00'): 0.3419231588248373, Timestamp('2017-04-01 00:00:00'): -2.7996228048180534, Timestamp('2017-07-01 00:00:00'): 0.6897907396430131, Timestamp('2017-10-01 00:00:00'): 1.0264525998033418, Timestamp('2018-01-01 00:00:00'): 0.5586072758166829, Timestamp('2018-04-01 00:00:00'): 0.6355686211759167, Timestamp('2018-07-01 00:00:00'): 0.43177414779181905, Timestamp('2018-10-01 00:00:00'): 0.13706256115290633, Timestamp('2019-01-01 00:00:00'): -0.7819957212531107, Timestamp('2019-04-01 00:00:00'): -1.482462546513092, Timestamp('2019-07-01 00:00:00'): 0.7339473515393193}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YSsNlwQPnPr",
        "colab_type": "text"
      },
      "source": [
        "## Pre-processing FOREX data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf33ycLPPufj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "startDate = '2018-01-01'\n",
        "endDate = '2019-01-01'\n",
        "\n",
        "# Get FOREX data\n",
        "quandl.ApiConfig.api_key = \"VXqfuyrbTE8xxYZzqePw\"\n",
        "dataGbpEurRate = quandl.get(\"BOE/XUDLERS\", start_date=START_DATE, returns=\"numpy\")\n",
        "forexDataN = dataGbpEurRate.Value\n",
        "\n",
        "# Normalise data\n",
        "forex_mean = forexDataN.mean()\n",
        "forex_std = forexDataN.std()\n",
        "forexDataN = (forexDataN - forex_mean) / forex_std\n",
        "\n",
        "forexData = {'Date':dataGbpEurRate.Date,'Value':forexDataN}\n",
        "forexDf = pd.DataFrame(forexData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1qXH4oOBAVu",
        "colab_type": "text"
      },
      "source": [
        "## FOREX Moving Averages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjWy9l5rBK58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_rows', 25)\n",
        "pd.set_option('display.max_columns', 25)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "\n",
        "ukFOREXdates = []\n",
        "for x in dataGbpEurRate.Date:\n",
        "    ukFOREXdates.append(pd.Timestamp(x))\n",
        "\n",
        "def getMovingAverages(data, windowSize):\n",
        "\n",
        "    movingAverages = []\n",
        "\n",
        "    for x in range(len(data)):\n",
        "        if (x < windowSize):\n",
        "            window = data[:x+1]\n",
        "        else:\n",
        "            window = data[x-(windowSize - 1):x+1]\n",
        "        \n",
        "        total = sum(window)\n",
        "        average = total / len(window)\n",
        "        movingAverages.append(average)\n",
        "\n",
        "    return movingAverages\n",
        "\n",
        "fiveDayMovingAverages = getMovingAverages(forexDataN, 5)\n",
        "tenDayMovingAverages = getMovingAverages(forexDataN, 10)\n",
        "\n",
        "fiveDayDict = {ukFOREXdates[i]: fiveDayMovingAverages[i] for i in range(len(fiveDayMovingAverages))}\n",
        "tenDayDict = {ukFOREXdates[i]: tenDayMovingAverages[i] for i in range(len(tenDayMovingAverages))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYtmcOd1QXf-",
        "colab_type": "text"
      },
      "source": [
        "## Creating full data matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaLD_2QagA8w",
        "colab_type": "code",
        "outputId": "24109a24-8c1d-45fa-96cb-8c5c076c4393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "pd.set_option('display.max_rows', 12)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "mainDf = pd.DataFrame(columns=['Date','ForexRate','5dayMovingAvg','10dayMovingAvg','CPIRatio', 'IRRatio', 'BOPRatio', 'OvrLIBOR', '1mLIBOR','3mLIBOR','6mLIBOR','12mLIBOR'])\n",
        "\n",
        "cpiCounter = 0\n",
        "irCounter = 0\n",
        "\n",
        "for index, row in forexDf.iterrows():\n",
        "\n",
        "    newD = row['Date']\n",
        "    roundD = newD.replace(day=1)\n",
        "    quarterD = 1\n",
        "\n",
        "    quarter = newD.quarter\n",
        "\n",
        "    cpi = cpiDict.get(roundD,0)\n",
        "    ir = irDict.get(roundD,0)\n",
        "\n",
        "    switcher={\n",
        "        1:newD.replace(month=1,day=1),\n",
        "        2:newD.replace(month=4,day=1),\n",
        "        3:newD.replace(month=7,day=1),\n",
        "        4:newD.replace(month=10,day=1)\n",
        "    }\n",
        "\n",
        "    quarterD = switcher.get(newD.quarter)\n",
        "\n",
        "    bop = bopDict.get(quarterD,0)\n",
        "\n",
        "    ovrI = GBPEURovrRatio.get(row['Date'], 0)\n",
        "    i1month = GBPEUR1mRatio.get(row['Date'], 0)\n",
        "    i3month = GBPEUR3mRatio.get(row['Date'], 0)\n",
        "    i6month = GBPEUR6mRatio.get(row['Date'], 0)\n",
        "    i12month = GBPEUR12mRatio.get(row['Date'], 0)\n",
        "\n",
        "    movingAvg5Day = fiveDayDict.get(row['Date'], 0)\n",
        "    movingAvg10Day = tenDayDict.get(row['Date'], 0)\n",
        "\n",
        "    mainDf = mainDf.append({'Date':row['Date'],\n",
        "                            'ForexRate':row['Value'],\n",
        "                            '5dayMovingAvg':movingAvg5Day,\n",
        "                            '10dayMovingAvg':movingAvg10Day,\n",
        "                            'CPIRatio': cpi,\n",
        "                            'IRRatio' : ir,\n",
        "                            'BOPRatio': bop,\n",
        "                            'OvrLIBOR': ovrI,\n",
        "                            '1mLIBOR': i1month,\n",
        "                            '3mLIBOR': i3month,\n",
        "                            '6mLIBOR': i6month,\n",
        "                            '12mLIBOR': i12month},\n",
        "                            ignore_index=True)\n",
        "\n",
        "\n",
        "#mainDf.set_index('Date')\n",
        "\n",
        "print(mainDf)\n",
        "\n",
        "# Split and format data\n",
        "futureDistance = 0\n",
        "historySize = 6"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           Date  ForexRate  5dayMovingAvg  10dayMovingAvg  CPIRatio   IRRatio  \\\n",
            "0    2016-01-04  3.788527   3.788527       3.788527       -0.27492  -1.691234   \n",
            "1    2016-01-05  3.907865   3.848196       3.848196       -0.27492  -1.691234   \n",
            "2    2016-01-06  3.782844   3.826412       3.826412       -0.27492  -1.691234   \n",
            "3    2016-01-07  3.511965   3.747801       3.747801       -0.27492  -1.691234   \n",
            "4    2016-01-08  3.280866   3.654414       3.654414       -0.27492  -1.691234   \n",
            "...         ...       ...        ...            ...            ...        ...   \n",
            "1052 2020-02-26  0.549346   0.624358       0.710547        0.00000   0.000000   \n",
            "1053 2020-02-27  0.231110   0.547073       0.650688        0.00000   0.000000   \n",
            "1054 2020-02-28  0.060627   0.428871       0.580411        0.00000   0.000000   \n",
            "1055 2020-03-02 -0.272762   0.258009       0.475280        0.00000   0.000000   \n",
            "1056 2020-03-03 -0.251926   0.063279       0.365224        0.00000   0.000000   \n",
            "\n",
            "      BOPRatio  OvrLIBOR   1mLIBOR   3mLIBOR   6mLIBOR  12mLIBOR  \n",
            "0    -0.625983 -1.571664 -2.526006 -4.296796 -9.442142  0.323226  \n",
            "1    -0.625983 -1.576820 -2.491253 -4.274812 -9.642618  0.321480  \n",
            "2    -0.625983 -1.533513 -2.484531 -4.131170 -8.217761  0.341000  \n",
            "3    -0.625983 -1.512108 -2.428298 -3.774799 -6.976580  0.365475  \n",
            "4    -0.625983 -1.554982 -2.441127 -3.774799 -6.743159  0.361381  \n",
            "...        ...       ...       ...       ...       ...       ...  \n",
            "1052  0.000000 -0.341222 -0.178921  0.243459  0.598229  0.064217  \n",
            "1053  0.000000 -0.267900 -0.123515  0.292877  0.613909  0.064731  \n",
            "1054  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1055  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "1056  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
            "\n",
            "[1057 rows x 12 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7V-Nx21IKt",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3pUL5P8rC_d",
        "colab_type": "text"
      },
      "source": [
        "## Standard ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFbpmT0sFOs",
        "colab_type": "code",
        "outputId": "be13b551-5580-4dc3-8582-6771acecaaf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from joblib import dump, load\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "def formatData(data, start, end, history, target):\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    startIndex = start + historySize\n",
        "    endIndex = len(data) - futureDistance\n",
        "\n",
        "    for i in range(startIndex, endIndex):\n",
        "        indices = range(i-historySize, i)\n",
        "        # Reshape data from (history_size,) to (history_size, 1)\n",
        "        x.append(data[indices])\n",
        "        y.append(data[i+futureDistance])\n",
        "        \n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "\n",
        "TRAIN_SPLIT = round(len(forexDataN) * 0.7)\n",
        "VALIDATION_SPLIT = round(len(forexDataN) * 0.85)\n",
        "\n",
        "xTrain, yTrain = formatData(forexDataN, 0, TRAIN_SPLIT, historySize, futureDistance)\n",
        "xVal, yVal = formatData(forexDataN, TRAIN_SPLIT, VALIDATION_SPLIT, historySize, futureDistance)\n",
        "xTest, yTest = formatData(forexDataN, VALIDATION_SPLIT, None, historySize, futureDistance)\n",
        "\n",
        "print(xTest[0])\n",
        "print(yTest[0])\n",
        "\n",
        "BATCH_SIZE = 30\n",
        "BUFFER_SIZE = 10\n",
        "\n",
        "# Form training tensors and shuffle etc.\n",
        "dataTrain = tf.data.Dataset.from_tensor_slices((xTrain, yTrain))\n",
        "dataTrain = dataTrain.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# Form validation tensors and shuffle etc.\n",
        "dataVal = tf.data.Dataset.from_tensor_slices((xVal, yVal))\n",
        "dataVal = dataVal.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "# Create model\n",
        "EVALUATION_INTERVAL = 10\n",
        "EPOCHS = 2\n",
        "\n",
        "def standard_ann_model():\n",
        "\tmodel = keras.Sequential([\n",
        "        layers.Dense(6,input_dim=(6),kernel_initializer='normal',activation='relu'),\n",
        "        layers.Dense(8, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "\tmodel.compile(optimizer='adam', loss='mse')\n",
        "\treturn model\n",
        "\n",
        "\n",
        "model = standard_ann_model()\n",
        "\n",
        "model.fit(dataTrain, epochs=EPOCHS,\n",
        "                    steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                    validation_data=dataVal,\n",
        "                    validation_steps=50\n",
        "          )\n",
        "\n",
        "\n",
        "model.save('newmodel.h5')\n",
        "\n",
        "result = model.evaluate(xTest, yTest, batch_size=30)\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"Model loss:\", result)\n",
        "\n",
        "\n",
        "#print(type(model))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.87324227 -0.83725136 -0.72170161 -0.76148104 -0.8808193  -1.15548674]\n",
            "-1.3051331341534052\n",
            "Train for 10 steps, validate for 50 steps\n",
            "Epoch 1/2\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 2.0597 - val_loss: 0.3751\n",
            "Epoch 2/2\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 0.3600 - val_loss: 0.3578\n",
            "153/153 [==============================] - 0s 380us/sample - loss: 0.3962\n",
            "-----------------------------------------\n",
            "Model loss: 0.3962322310635857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIg8KNPOqiyn",
        "colab_type": "text"
      },
      "source": [
        "## Univariate LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HeS-tpxf97i",
        "colab_type": "code",
        "outputId": "92c634db-87aa-49ee-c300-5a6591a83542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "def formatData(data, start, end, history, target):\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    startIndex = start + historySize---*-\n",
        "    endIndex = len(data) - futureDistance\n",
        "\n",
        "    for i in range(startIndex, endIndex):\n",
        "        indices = range(i-historySize, i)\n",
        "        # Reshape data from (history_size,) to (history_size, 1)\n",
        "        x.append(np.reshape(data[indices], (historySize, 1)))\n",
        "        y.append(data[i+futureDistance])\n",
        "        \n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "TRAIN_SPLIT = round(len(forexDataN) * 0.7)\n",
        "VALIDATION_SPLIT = round(len(forexDataN) * 0.85)\n",
        "\n",
        "xTrain, yTrain = formatData(forexDataN, 0, TRAIN_SPLIT, historySize, futureDistance)\n",
        "xVal, yVal = formatData(forexDataN, TRAIN_SPLIT, VALIDATION_SPLIT, historySize, futureDistance)\n",
        "xTest, yTest = formatData(forexDataN, VALIDATION_SPLIT, None, historySize, futureDistance)\n",
        "\n",
        "BATCH_SIZE = 30\n",
        "BUFFER_SIZE = 10\n",
        "\n",
        "# Form training tensors and shuffle etc.\n",
        "dataTrain = tf.data.Dataset.from_tensor_slices((xTrain, yTrain))\n",
        "dataTrain = dataTrain.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "# Form validation tensors and shuffle etc.\n",
        "dataVal = tf.data.Dataset.from_tensor_slices((xVal, yVal))\n",
        "dataVal = dataVal.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "# Create model\n",
        "EVALUATION_INTERVAL = 100\n",
        "EPOCHS = 10\n",
        "\n",
        "def lstm_ann_model():\n",
        "    lstm_model = keras.Sequential([\n",
        "        layers.LSTM(8, input_shape=xTrain.shape[-2:]),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    lstm_model.compile(optimizer='adam', loss='mse')\n",
        "    return lstm_model\n",
        "\n",
        "model = lstm_ann_model()\n",
        "\n",
        "model.fit(dataTrain, epochs=EPOCHS,\n",
        "                     steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                      validation_data=dataVal,\n",
        "                      validation_steps=50)\n",
        "\n",
        "\n",
        "\n",
        "result = model.evaluate(xTest, yTest, batch_size=30)\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"Model loss:\", result)\n",
        "\n",
        "-"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 100 steps, validate for 50 steps\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 1.5942 - val_loss: 0.5296\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 1.1193 - val_loss: 0.3342\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.7014 - val_loss: 0.1739\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.3484 - val_loss: 0.0810\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.1361 - val_loss: 0.0634\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.1335 - val_loss: 0.0630\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.0929 - val_loss: 0.0624\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.0708 - val_loss: 0.0611\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.0596 - val_loss: 0.0572\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 0s 4ms/step - loss: 0.0618 - val_loss: 0.0540\n",
            "153/153 [==============================] - 0s 2ms/sample - loss: 0.0702\n",
            "-----------------------------------------\n",
            "Model loss: 0.07023120459680464\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqgN112q0mGA",
        "colab_type": "text"
      },
      "source": [
        "## Multivariate LSTM (Single-step)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P4-NPkw0rVV",
        "colab_type": "code",
        "outputId": "299c60da-3528-47dc-cf13-2c24d02552c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "EVALUATION_INTERVAL = 100\n",
        "\n",
        "\n",
        "features = ['ForexRate', 'CPIRatio', 'OvrLIBOR']\n",
        "\n",
        "dataSet = mainDf[features]\n",
        "dataSet.index = mainDf['Date']\n",
        "\n",
        "dataSet = dataSet.values\n",
        "\n",
        "TRAIN_SPLIT = round(len(dataSet) * 0.7)\n",
        "VALIDATION_SPLIT = round(len(dataSet) * 0.85)\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "def getData(dataSet,target,start,end,lags,targetSize,step):\n",
        "\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    start = start + lags\n",
        "    end = end - futureDistance\n",
        "\n",
        "    for i in range(start,end):\n",
        "        indices = range(i-lags, i, step)\n",
        "        data.append(dataSet[indices])\n",
        "        labels.append(target[i+targetSize])\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "lags = 20\n",
        "futureDistance = 10\n",
        "step = 25\n",
        "\n",
        "xTrain, yTrain = getData(dataSet, dataSet[:, 0], 0, TRAIN_SPLIT, lags, futureDistance, step)\n",
        "xVal, yVal = getData(dataSet, dataSet[:, 0],TRAIN_SPLIT, VALIDATION_SPLIT, lags, futureDistance, step)\n",
        "xTest, yTest = getData(dataSet, dataSet[:, 0],VALIDATION_SPLIT, len(dataSet), lags, futureDistance, step)\n",
        "\n",
        "trainData = tf.data.Dataset.from_tensor_slices((xTrain, yTrain))\n",
        "trainData = trainData.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "valData = tf.data.Dataset.from_tensor_slices((xVal, yVal))\n",
        "valData = valData.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "def lstm_ann_single_model():\n",
        "    lstm_model = keras.Sequential([\n",
        "        layers.LSTM(32, input_shape=xTrain.shape[-2:]),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    lstm_model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mse')\n",
        "    return lstm_model\n",
        "\n",
        "\n",
        "model = lstm_ann_single_model()\n",
        "\n",
        "single_step_history = model.fit(trainData, epochs=EPOCHS,\n",
        "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                                            validation_data=valData,\n",
        "                                            validation_steps=50)\n",
        "\n",
        "result = model.evaluate(xTest, yTest, batch_size=30)\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"Model loss:\", result)\n",
        "-"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 100 steps, validate for 50 steps\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.9968 - val_loss: 0.2701\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.6165 - val_loss: 0.2463\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.4300 - val_loss: 0.2220\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.3068 - val_loss: 0.2005\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2481 - val_loss: 0.1884\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2320 - val_loss: 0.1812\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2391 - val_loss: 0.1842\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2320 - val_loss: 0.1854\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2222 - val_loss: 0.1854\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 0.2200 - val_loss: 0.1849\n",
            "129/129 [==============================] - 0s 2ms/sample - loss: 0.2272\n",
            "-----------------------------------------\n",
            "Model loss: 0.22723477247149446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnQ1W8Gtcwaf",
        "colab_type": "text"
      },
      "source": [
        "##Multivariate LSTM (Multi-step)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZPKbXIMg2hp",
        "colab_type": "code",
        "outputId": "4f04c482-fef1-4424-8348-0ff024820ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "future_target = 10\n",
        "lags = 50\n",
        "step = 1\n",
        "EPOCHS = 2\n",
        "EVALUATION_INTERVAL = 10\n",
        "\n",
        "xTrainM, yTrainM = getData(dataSet, dataSet[:, 1], 0, TRAIN_SPLIT, lags, future_target, step)\n",
        "xValM, yValM = getData(dataSet, dataSet[:, 1], TRAIN_SPLIT, VALIDATION_SPLIT, lags, future_target, step)\n",
        "\n",
        "trainDataM = tf.data.Dataset.from_tensor_slices((xTrainM, yTrainM))\n",
        "trainDataM = trainDataM.cache().batch(BATCH_SIZE).repeat()\n",
        "valDataM = tf.data.Dataset.from_tensor_slices((xValM, yValM))\n",
        "valDataM = valDataM.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "def lstm_ann_multi_model():\n",
        "    lstm_model = keras.Sequential([\n",
        "        layers.LSTM(units=32,return_sequences=True,input_shape = (xTrainM.shape[1], xTrainM.shape[2])),\n",
        "        layers.LSTM(16, activation='relu'),\n",
        "        layers.Dense(future_target)\n",
        "    ])\n",
        "\n",
        "    lstm_model.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mae')\n",
        "    return lstm_model\n",
        "\n",
        "model = lstm_ann_multi_model()\n",
        "multi_history = model.fit(trainDataM, epochs=EPOCHS,steps_per_epoch=EVALUATION_INTERVAL,validation_data=valDataM,validation_steps=50)\n",
        "\n",
        "model.predict(x)[0]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 10 steps, validate for 50 steps\n",
            "Epoch 1/2\n",
            "10/10 [==============================] - 3s 334ms/step - loss: 0.6265 - val_loss: 0.1472\n",
            "Epoch 2/2\n",
            "10/10 [==============================] - 1s 76ms/step - loss: 0.2255 - val_loss: 0.1419\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.46661434, -0.27093875, -0.3945876 ,  0.25004852,  0.04249762,\n",
              "       -0.3015085 , -0.07858429, -0.20816644, -0.25933644, -0.34645557],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDc-oKOp1T6O",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CxChDHwvxdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def create_time_steps(length):\n",
        "  return list(range(-length, 0))\n",
        "\n",
        "def show_plot(plot_data, delta, title):\n",
        "  labels = ['History', 'True Future', 'Model Prediction']\n",
        "  marker = ['.-', 'rx', 'go']\n",
        "  time_steps = create_time_steps(plot_data[0].shape[0])\n",
        "  if delta:\n",
        "    future = delta\n",
        "  else:\n",
        "    future = 0\n",
        "\n",
        "  plt.title(title)\n",
        "  for i, x in enumerate(plot_data):\n",
        "    if i:\n",
        "      plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
        "               label=labels[i])\n",
        "    else:\n",
        "      plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "  plt.legend()\n",
        "  plt.xlim([time_steps[0], (future+5)*2])\n",
        "  plt.xlabel('Time-Step')\n",
        "  return plt\n",
        "\n",
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(len(history))\n",
        "  num_out = len(true_future)\n",
        "\n",
        "  plt.plot(num_in, np.array(history[:, 1]), label='History')\n",
        "  plt.plot(np.arange(num_out)/STEP, np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out)/STEP, np.array(prediction), 'ro',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "#for x, y in valData.take(3):\n",
        "#  plot = show_plot([x[0][:, 0].numpy(), y[0].numpy(),\n",
        "#                    model.predict(x)[0]], 12,\n",
        "#                   'Single Step Prediction')\n",
        "#  plot.show()\n",
        "\n",
        "#for x, y in valDataM.take(3):\n",
        "#    multi_step_plot(x[0], y[0], model.predict(x)[0])\n",
        "\n",
        "for x, y in valDataM.take(3):\n",
        "  multi_step_plot(x[0], y[0], model.predict(x)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}