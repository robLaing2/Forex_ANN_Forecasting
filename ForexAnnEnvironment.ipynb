{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ForexAnnEnvironment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1Y23CLo2oUco-O4CAkKZhYsjzzzbnTeLP",
      "authorship_tag": "ABX9TyMQdHSTTTHqHPXf5Z1NDIxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robLaing2/Forex_ANN_Forecasting/blob/master/ForexAnnEnvironment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo0f_HWfsWuF",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIPPZ27ksNag",
        "colab_type": "code",
        "outputId": "50f513ab-c5c3-4882-f349-8a58250639e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "!pip install quandl\n",
        "!pip install dbnomics\n",
        "#!pip install FRB\n",
        "!pip install fred\n",
        "!pip install mock\n",
        "#!pip uninstall tensorflow\n",
        "#!pip install tensorflow==2.0.0\n",
        "\n",
        "import fred\n",
        "from mock import Mock\n",
        "import requests\n",
        "import json\n",
        "import quandl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, CuDNNLSTM\n",
        "from dbnomics import fetch_series\n",
        "import pandas as pd\n",
        "from keras.models import model_from_json\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: quandl in /usr/local/lib/python3.6/dist-packages (3.5.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl) (8.2.0)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from quandl) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from quandl) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.18.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from quandl) (2.8.1)\n",
            "Requirement already satisfied: inflection>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.4.0)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.0.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2020.4.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Requirement already satisfied: dbnomics in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.6/dist-packages (from dbnomics) (2.21.0)\n",
            "Requirement already satisfied: pandas>=0.21 in /usr/local/lib/python3.6/dist-packages (from dbnomics) (1.0.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->dbnomics) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->dbnomics) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->dbnomics) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18.4->dbnomics) (2020.4.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->dbnomics) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->dbnomics) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21->dbnomics) (1.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.21->dbnomics) (1.12.0)\n",
            "Requirement already satisfied: fred in /usr/local/lib/python3.6/dist-packages (3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fred) (2.21.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fred) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fred) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fred) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fred) (3.0.4)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.6/dist-packages (4.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knovwSza04MP",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSX-AVfVGK6s",
        "colab_type": "code",
        "outputId": "e1093e58-f8e4-44fb-b49c-fcfd206a5bea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "START_DATE = '2001-01-01'\n",
        "END_DATE = '2020-02-01'\n",
        "\n",
        "pd.set_option('display.max_rows', 25)\n",
        "pd.set_option('display.max_columns', 25)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mBJkhEqGfRq",
        "colab_type": "text"
      },
      "source": [
        "## Moving average function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpuooBtRGero",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getMovingAverages(data, windowSize):\n",
        "\n",
        "    movingAverages = []\n",
        "\n",
        "    for x in range(len(data)):\n",
        "        if (x < windowSize):\n",
        "            window = data[:x+1]\n",
        "        else:\n",
        "            window = data[x-(windowSize - 1):x+1]\n",
        "        \n",
        "        total = sum(window)\n",
        "        average = total / len(window)\n",
        "        movingAverages.append(average)\n",
        "\n",
        "    return movingAverages"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n68WYYVB6UK7",
        "colab_type": "text"
      },
      "source": [
        "## FOREX data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf33ycLPPufj",
        "colab_type": "code",
        "outputId": "89cd8816-0cde-4c7f-cec9-1a65e2b7e39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Get FOREX data\n",
        "quandl.ApiConfig.api_key = \"VXqfuyrbTE8xxYZzqePw\"\n",
        "dataGbpEurRate = quandl.get(\"BOE/XUDLERS\", start_date=START_DATE, end_date=END_DATE, returns=\"numpy\")\n",
        "forexDataN = dataGbpEurRate.Value\n",
        "\n",
        "forexMonthMovAvg = getMovingAverages(forexDataN, 22)\n",
        "forexMonthMovAvg = np.asarray(forexMonthMovAvg)\n",
        "\n",
        "# Normalise data\n",
        "forex_mean = forexMonthMovAvg.mean()\n",
        "forex_std = forexMonthMovAvg.std()\n",
        "forexMonthMovAvg = (forexMonthMovAvg - forex_mean) / forex_std\n",
        "\n",
        "ukFOREXdates = []\n",
        "for x in dataGbpEurRate.Date:\n",
        "    ukFOREXdates.append(pd.Timestamp(x))\n",
        "\n",
        "forexData = {'Date':ukFOREXdates,'Value':forexMonthMovAvg}\n",
        "mainDf = pd.DataFrame(forexData)\n",
        "\n",
        "print(forex_mean)\n",
        "print(forex_std)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.309593875180065\n",
            "0.16475970732268408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrnOCJajGrND",
        "colab_type": "text"
      },
      "source": [
        "## Interest Rate Data (INT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUTeDp07W41L",
        "colab_type": "text"
      },
      "source": [
        "### INT data retreival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0f5HNs21pcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "GBPovr = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBPONTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE + '&observation_end='+ END_DATE)\n",
        "EURovr = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EURONTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE + '&observation_end='+ END_DATE)\n",
        "GBP1month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP1MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE + '&observation_end='+ END_DATE)\n",
        "EUR1month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR1MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start=' + START_DATE + '&observation_end='+ END_DATE)\n",
        "GBP3month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP3MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE + '&observation_end='+ END_DATE)\n",
        "EUR3month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR3MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE + '&observation_end='+ END_DATE)\n",
        "GBP6month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP6MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE + '&observation_end='+ END_DATE)\n",
        "EUR6month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR6MTD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE + '&observation_end='+ END_DATE)\n",
        "GBP12month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=GBP12MD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE + '&observation_end='+ END_DATE)\n",
        "EUR12month = requests.get('https://api.stlouisfed.org/fred/series/observations?series_id=EUR12MD156N&api_key=cdc4dd9f1b0596f6209a77cec5453528&file_type=json&observation_start='+ START_DATE + '&observation_end='+ END_DATE)\n",
        "\n",
        "GBRovrJson = (json.loads(GBPovr.content))[\"observations\"]\n",
        "EURovrJson = (json.loads(EURovr.content))[\"observations\"]\n",
        "GBR1mJson = (json.loads(GBP1month.content))[\"observations\"]\n",
        "EUR1mJson = (json.loads(EUR1month.content))[\"observations\"]\n",
        "GBR3mJson = (json.loads(GBP3month.content))[\"observations\"]\n",
        "EUR3mJson = (json.loads(EUR3month.content))[\"observations\"]\n",
        "GBR6mJson = (json.loads(GBP6month.content))[\"observations\"]\n",
        "EUR6mJson = (json.loads(EUR6month.content))[\"observations\"]\n",
        "GBR12mJson = (json.loads(GBP12month.content))[\"observations\"]\n",
        "EUR12mJson = (json.loads(EUR12month.content))[\"observations\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUgecCRUBkiR",
        "colab_type": "text"
      },
      "source": [
        "### INT data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkkAjnFLBlhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanDataSets(dataset):\n",
        "\n",
        "    dataDict = {pd.Timestamp(dataset[i][\"date\"]): dataset[i][\"value\"] for i in range(len(dataset))}\n",
        "    cleanedDataDict= {}\n",
        "    count = 0\n",
        "\n",
        "    for index, row in mainDf.iterrows():\n",
        "        value = dataDict.get(row['Date'], 1000000)\n",
        "\n",
        "        if (value=='.'):\n",
        "            value = 1000000\n",
        "\n",
        "        if(value==1000000):\n",
        "            dateBelow = mainDf.Date.iloc[index-1]\n",
        "            dateAbove = mainDf.Date.iloc[index+1]\n",
        "\n",
        "            valueBelow = dataDict.get(dateBelow, 1000000)\n",
        "            valueAbove = dataDict.get(dateAbove, 1000000)\n",
        "\n",
        "            average = (float(valueBelow) + float(valueAbove)) / 2\n",
        "\n",
        "            value = average\n",
        "\n",
        "        cleanedDataDict[row['Date']] = value\n",
        "        count = count + 1\n",
        "\n",
        "    return cleanedDataDict\n",
        "\n",
        "GBRovrC = cleanDataSets(GBRovrJson)\n",
        "EURovrC = cleanDataSets(EURovrJson)\n",
        "GBR3mC = cleanDataSets(GBR3mJson)\n",
        "EUR3mC = cleanDataSets(EUR3mJson)\n",
        "GBR6mC = cleanDataSets(GBR6mJson)\n",
        "EUR6mC = cleanDataSets(EUR6mJson)\n",
        "GBR12mC = cleanDataSets(GBR12mJson)\n",
        "EUR12mC = cleanDataSets(EUR12mJson)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euiAqH5oXFmC",
        "colab_type": "text"
      },
      "source": [
        "### INT feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zjzi_yf1sjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDifferenceFeatures(xDict, yDict):\n",
        "    dates = []\n",
        "    valuesX = []\n",
        "    valuesY = []\n",
        "    ratioValues = []                      \n",
        "\n",
        "    for k,v in xDict.items():\n",
        "\n",
        "        match = yDict.get(k, 0)\n",
        "        valuesX.append(float(v))\n",
        "        valuesY.append(float(match))\n",
        "        dates.append(k)\n",
        " \n",
        "    datasetXarr = np.array(valuesX, dtype=np.float)\n",
        "    datasetYarr = np.array(valuesY, dtype=np.float)\n",
        "\n",
        "    diffValues = datasetXarr - datasetYarr\n",
        "\n",
        "    movingAvg = getMovingAverages(diffValues, 22)\n",
        "    movingAvg = np.asarray(movingAvg)\n",
        "\n",
        "    data_mean = movingAvg.mean()\n",
        "    data_std = movingAvg.std()\n",
        "    dataNormalised = (movingAvg - data_mean) - data_std\n",
        "\n",
        "    res = {dates[i]: dataNormalised[i] for i in range(len(dates))}\n",
        "\n",
        "    return res\n",
        "\n",
        "ovrRatioMovAvg = getDifferenceFeatures(GBRovrC,EURovrC)\n",
        "threeMRatioMovAvg = getDifferenceFeatures(GBR3mC,EUR3mC)\n",
        "sixMRatioMovAvg = getDifferenceFeatures(GBR6mC,EUR6mC)\n",
        "twelveMRatioMovAvg = getDifferenceFeatures(GBR12mC,EUR12mC)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4pR6Orq9ma",
        "colab_type": "text"
      },
      "source": [
        "## Inflation data (CPI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbUCPuwpYOnn",
        "colab_type": "text"
      },
      "source": [
        "### CPI data retreival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wVanjbf-n1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ukCPI = fetch_series('IMF/CPI/M.GB.PCPIHA_PC_CP_A_PT')\n",
        "euCPI = fetch_series('IMF/CPI/M.U2.PCPIHA_PC_CP_A_PT')\n",
        "\n",
        "dbnomicsQuery = \"period >= '\" + START_DATE + \"'\"\n",
        "\n",
        "ukCPI = ukCPI.query(dbnomicsQuery)\n",
        "euCPI = euCPI.query(dbnomicsQuery)\n",
        "\n",
        "ukCPIDict = {ukCPI.period.iloc[i]: ukCPI.value.iloc[i] for i in range(len(ukCPI))}\n",
        "euCPIDict = {euCPI.period.iloc[i]: euCPI.value.iloc[i] for i in range(len(euCPI))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhffkl2I-kY0",
        "colab_type": "text"
      },
      "source": [
        "### CPI data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRxREJ3rFVj5",
        "colab_type": "code",
        "outputId": "cde0ec1c-6b7e-41c6-d53b-e38356cbcf4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def cleanMonthlyData(dataset):\n",
        "\n",
        "    cleanedDataDict= {}\n",
        "    count = 0\n",
        "    clean=True\n",
        "\n",
        "    for index, row in mainDf.iterrows():\n",
        "\n",
        "        roundD = row['Date'].replace(day=1)\n",
        "\n",
        "        value= dataset.get(pd.Timestamp(roundD),1000000)\n",
        "\n",
        "        if(value==1000000):\n",
        "            clean = False\n",
        "            dateBelow = mainDf.Date.iloc[index-1]\n",
        "            dateAbove = mainDf.Date.iloc[index+1]\n",
        "\n",
        "            valueBelow = dataset.get(dateBelow, 1000000)\n",
        "            valueAbove = dataset.get(dateAbove, 1000000)\n",
        "\n",
        "            average = (float(valueBelow) + float(valueAbove)) / 2\n",
        "\n",
        "            value = average\n",
        "\n",
        "        cleanedDataDict[row['Date']] = value\n",
        "        count = count + 1\n",
        "\n",
        "    if(clean==True):\n",
        "        print(\"Data is clean\")\n",
        "\n",
        "    return cleanedDataDict\n",
        "\n",
        "\n",
        "ukCPIDictC = cleanMonthlyData(ukCPIDict)\n",
        "euCPIDictC = cleanMonthlyData(euCPIDict)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data is clean\n",
            "Data is clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T2dZu4RA6Sl",
        "colab_type": "text"
      },
      "source": [
        "### CPI feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBLa-rIMNPut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dates = []\n",
        "ukCPIarr = []\n",
        "euCPIarr = []\n",
        "\n",
        "for k,v in ukCPIDictC.items():\n",
        "\n",
        "    match = euCPIDictC.get(k, 0)\n",
        "\n",
        "    ukCPIarr.append(v)\n",
        "    euCPIarr.append(match)\n",
        "    dates.append(k)\n",
        "\n",
        "ukCPIarr = np.array(ukCPIarr, dtype=np.float)\n",
        "euCPIarr = np.array(euCPIarr, dtype=np.float)\n",
        "\n",
        "ukEuCpiRatio = ukCPIarr - euCPIarr\n",
        "\n",
        "# Normalise CPI data\n",
        "cpi_mean = ukEuCpiRatio.mean()\n",
        "cpi_std = ukEuCpiRatio.std()\n",
        "\n",
        "ukEuCpiRatio = (ukEuCpiRatio - cpi_mean) / cpi_std\n",
        "\n",
        "cpiDict = {dates[i]: ukEuCpiRatio[i] for i in range(len(dates))}\n",
        "\n",
        "cpiData = {'Date':dates, 'Value':ukEuCpiRatio}\n",
        "cpiDf = pd.DataFrame(cpiData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz9f5MPUOBva",
        "colab_type": "text"
      },
      "source": [
        "## International Reserves data (IR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeoJsXNEQA_m",
        "colab_type": "text"
      },
      "source": [
        "### IR data retreival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXRsvqQIQMG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ukIR = fetch_series('IMF/IFS/M.GB.RAFAGOLDM_USD')\n",
        "euIR = fetch_series('IMF/IFS/M.U2.RAFAGOLDM_USD')\n",
        "\n",
        "dbnomicsQuery = \"period >= '\" + START_DATE + \"'\"\n",
        "\n",
        "ukIR = ukIR.query(dbnomicsQuery)\n",
        "euIR = euIR.query(dbnomicsQuery)\n",
        "\n",
        "ukIRDict = {ukIR.period.iloc[i]: ukIR.value.iloc[i] for i in range(len(ukIR))}\n",
        "euIRDict = {euIR.period.iloc[i]: euIR.value.iloc[i] for i in range(len(euIR))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj0vLPaBQFde",
        "colab_type": "text"
      },
      "source": [
        "### IR data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZbMYaFhQREG",
        "colab_type": "code",
        "outputId": "ef3459f8-3871-4727-b803-e4e459de2efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "ukIRDictC = cleanMonthlyData(ukIRDict)\n",
        "euIRDictC = cleanMonthlyData(euIRDict)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data is clean\n",
            "Data is clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCU1PotbQH3d",
        "colab_type": "text"
      },
      "source": [
        "### IR feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu_mv-VUOMyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IRdates = []\n",
        "ukIRarr = []\n",
        "euIRarr = []\n",
        "\n",
        "for k,v in ukIRDictC.items():\n",
        "\n",
        "    match = euIRDictC.get(k, 0)\n",
        "\n",
        "    ukIRarr.append(v)\n",
        "    euIRarr.append(match)\n",
        "    IRdates.append(k)\n",
        "\n",
        "\n",
        "ukIRarr = np.array(ukIRarr, dtype=np.float)\n",
        "euIRarr = np.array(euIRarr, dtype=np.float)\n",
        "\n",
        "ukEuIRRatio = ukIRarr / euIRarr\n",
        "\n",
        "ir_mean = ukEuIRRatio.mean()\n",
        "ir_std = ukEuIRRatio.std()\n",
        "ukEuIRRatio = (ukEuIRRatio - ir_mean) / ir_std\n",
        "\n",
        "irDict = {IRdates[i]: ukEuIRRatio[i] for i in range(len(IRdates))}\n",
        "\n",
        "irData = {'Date':IRdates, 'Value':ukEuIRRatio}\n",
        "irDf = pd.DataFrame(irData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVW8UDb5OziA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Balance of Payments data (BOP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FpSuRNLR8Rb",
        "colab_type": "text"
      },
      "source": [
        "### BOP data retreival"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ_2Bg9PSEym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ukBOP = fetch_series('IMF/BOP/Q.GB.BACK_BP6_USD')\n",
        "euBOP = fetch_series('IMF/BOP/Q.U2.BACK_BP6_USD')\n",
        "\n",
        "dbnomicsQuery = \"period >= '\" + START_DATE + \"'\"\n",
        "\n",
        "ukBOP = ukBOP.query(dbnomicsQuery)\n",
        "euBOP = euBOP.query(dbnomicsQuery)\n",
        "\n",
        "ukBOPDict = {ukBOP.period.iloc[i]: ukBOP.value.iloc[i] for i in range(len(ukBOP))}\n",
        "euBOPDict = {euBOP.period.iloc[i]: euBOP.value.iloc[i] for i in range(len(euBOP))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EsjDOHjR_Q-",
        "colab_type": "text"
      },
      "source": [
        "### BOP data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsVJ5XKGSmu_",
        "colab_type": "code",
        "outputId": "4d36a4ac-9567-4490-8fa0-5fbc5889050d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def cleanQuarterlyData(dataset):\n",
        "\n",
        "    cleanedDataDict= {}\n",
        "    count = 0\n",
        "    clean=True\n",
        "\n",
        "    for index, row in mainDf.iterrows():\n",
        "\n",
        "        date = row['Date']\n",
        "        dateMonth = date.replace(day=1)\n",
        "        dateQuarter = date.quarter\n",
        "        \n",
        "        switcher={\n",
        "            1:date.replace(month=1,day=1),\n",
        "            2:date.replace(month=4,day=1),\n",
        "            3:date.replace(month=7,day=1),\n",
        "            4:date.replace(month=10,day=1)\n",
        "        }\n",
        "\n",
        "        dateRoundedQuarter = switcher.get(dateQuarter)\n",
        "\n",
        "        value = dataset.get(dateRoundedQuarter,1000000)\n",
        "\n",
        "        if(value==1000000):\n",
        "            mainDf.drop([index], inplace=True)\n",
        "        else:\n",
        "            cleanedDataDict[row['Date']] = value\n",
        "\n",
        "\n",
        "    clean = True\n",
        "    for k,v in cleanedDataDict.items():\n",
        "        if (v==1000000):\n",
        "            clean = False;\n",
        "\n",
        "    if (clean==False):\n",
        "        print(\"Data is unlcean\")\n",
        "    else:\n",
        "        print(\"Data is clean\")\n",
        "\n",
        "\n",
        "    return cleanedDataDict\n",
        "\n",
        "\n",
        "ukBOPDictC = cleanQuarterlyData(ukBOPDict)\n",
        "euBOPDictC = cleanQuarterlyData(euBOPDict)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data is clean\n",
            "Data is clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW5T04eISB7G",
        "colab_type": "text"
      },
      "source": [
        "### BOP feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYGfT2feO9Yb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BOPdates = []\n",
        "ukBOParr = []\n",
        "euBOParr = []\n",
        "\n",
        "for k,v in ukBOPDictC.items():\n",
        "\n",
        "    match = euBOPDictC.get(k, 0)\n",
        "\n",
        "    ukBOParr.append(v)\n",
        "    euBOParr.append(match)\n",
        "    BOPdates.append(k)\n",
        "\n",
        "ukBOParr = np.array(ukBOParr, dtype=np.float)\n",
        "euBOParr = np.array(euBOParr, dtype=np.float)\n",
        "\n",
        "ukEuBOPRatio = ukBOParr / euBOParr\n",
        "\n",
        "# Normalise BOP data\n",
        "bop_mean = ukEuBOPRatio.mean()\n",
        "bop_std = ukEuBOPRatio.std()\n",
        "ukEuBOPRatio = (ukEuBOPRatio - bop_mean) / bop_std\n",
        "\n",
        "bopDict = {BOPdates[i]: ukEuBOPRatio[i] for i in range(len(BOPdates))}\n",
        "\n",
        "bopData = {'Date':BOPdates, 'Value':ukEuBOPRatio}\n",
        "bopDf = pd.DataFrame(bopData)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYtmcOd1QXf-",
        "colab_type": "text"
      },
      "source": [
        "## Creating full data matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STOlDl_FgRI8",
        "colab_type": "code",
        "outputId": "23cfa041-2a74-4364-c597-6b49a6e8534c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "completeDf = pd.DataFrame(columns=['Date','ForexRate','CPIRatio', 'IRRatio', 'BOPRatio', 'OvrLIBOR','3mLIBOR','6mLIBOR','12mLIBOR'])\n",
        "\n",
        "cpiCounter = 0\n",
        "irCounter = 0\n",
        " \n",
        "for index, row in mainDf.iterrows():\n",
        "\n",
        "    date = row['Date']\n",
        "    forex = row['Value']\n",
        "    \n",
        "    cpi = cpiDict.get(date, 0)\n",
        "    ir = irDict.get(date,0)\n",
        "    bop = bopDict.get(date,0)\n",
        "\n",
        "    ovrI = ovrRatioMovAvg.get(date, 0)\n",
        "    i3month = threeMRatioMovAvg.get(date, 0)\n",
        "    i6month = sixMRatioMovAvg.get(date, 0)\n",
        "    i12month = twelveMRatioMovAvg.get(date, 0)\n",
        "\n",
        "    completeDf = completeDf.append({'Date':date,\n",
        "                            'ForexRate':forex,\n",
        "                            'CPIRatio': cpi,\n",
        "                            'IRRatio' : ir,\n",
        "                            'BOPRatio': bop,\n",
        "                            'OvrLIBOR': ovrI,\n",
        "                            '1mLIBOR': i1month,\n",
        "                            '3mLIBOR': i3month,\n",
        "                            '6mLIBOR': i6month,\n",
        "                            '12mLIBOR': i12month},\n",
        "                            ignore_index=True)\n",
        "\n",
        "#print(completeDf)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  after removing the cwd from sys.path.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9gVEULcO0zD",
        "colab_type": "text"
      },
      "source": [
        "# Variable Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpiLy6YcO5-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "variables = ['CPIRatio', 'IRRatio', 'BOPRatio', 'OvrLIBOR','3mLIBOR','6mLIBOR','12mLIBOR']\n",
        "\n",
        "forex = completeDf['ForexRate'].tolist()\n",
        "correlations = []\n",
        "\n",
        "for x in range(len(variables)):\n",
        "    \n",
        "    column = completeDf[variables[x]].tolist()\n",
        "\n",
        "    r = np.corrcoef(forex, column)\n",
        "\n",
        "    correlations.append(r[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD7MnSU94FOw",
        "colab_type": "code",
        "outputId": "c4e69823-8031-4101-cf5d-7da4a98cbe97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "y_pos = np.arange(0,14,2)\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.bar(y_pos, correlations, align='center', alpha=0.5, color=(0.0, 0.0, 0.0, 1))\n",
        "plt.xticks(y_pos, variables)\n",
        "plt.ylabel('Usage')\n",
        "plt.title('Correlations')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuMlG3PXw4dX",
        "colab_type": "text"
      },
      "source": [
        "# Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxSvQDGXwFVJ",
        "colab_type": "text"
      },
      "source": [
        "## Data setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4_5GXRVwHR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "EPOCHS = 10\n",
        "EVALUATION_INTERVAL = 100\n",
        "VALIDATION_STEPS = 50\n",
        "BATCH_SIZE = 30\n",
        "FOLDS = 5\n",
        "\n",
        "HISTORY_STEPS = 20\n",
        "FUTURE_STEPS = 6\n",
        "\n",
        "features = ['ForexRate']\n",
        "\n",
        "dataSet = completeDf[features]\n",
        "dataSet = dataSet.values\n",
        "\n",
        "fold_steps = math.floor(len(dataSet) / FOLDS)\n",
        "fold_locations = []\n",
        "results = []\n",
        "\n",
        "for x in range(0,len(dataSet), fold_steps):\n",
        "    fold_locations.append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4r-1IZnpzKj",
        "colab_type": "text"
      },
      "source": [
        "## Data splitting functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGzFM320p1Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getIndices(currentIndex, steps):\n",
        "\n",
        "    indices = []\n",
        "\n",
        "    index = currentIndex\n",
        "\n",
        "    for i in range(1, steps + 1):\n",
        "        if i % 4 == 0:\n",
        "            indices.append(index)\n",
        "            index = index - 21\n",
        "        else:\n",
        "            indices.append(index)\n",
        "            index = index - 22\n",
        "\n",
        "    indices = list(reversed(indices))\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def singleStepDataSplit(dataset, target, startIndex, endIndex,\n",
        "                steps, future_steps):  \n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    history_size = 22 * steps\n",
        "\n",
        "    max_index = 22 * future_steps\n",
        "    target_size = round(21.75 * future_steps)\n",
        "\n",
        "    startIndex = startIndex + history_size\n",
        "\n",
        "    if endIndex is None:\n",
        "        endIndex = len(dataset) - max_index\n",
        "\n",
        "    for i in range(startIndex, endIndex):\n",
        "        dataIndices = getIndices(i,STEPS)\n",
        "        data.append(dataset[dataIndices])\n",
        "        labels.append(target[i+target_size])\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "def getFutureIndices(currentIndex, steps):\n",
        "\n",
        "    indices = []\n",
        "\n",
        "    index = currentIndex + 22\n",
        "\n",
        "    for i in range(1, steps + 1):\n",
        "        if i % 4 == 0:\n",
        "            indices.append(index)\n",
        "            index = index + 21\n",
        "        else:\n",
        "            indices.append(index)\n",
        "            index = index + 22\n",
        "\n",
        "    return indices\n",
        "\n",
        "\n",
        "def splitData(dataset, target, start_index, end_index, steps, future_steps):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    history_size = 22 * steps\n",
        "    target_size = 22 * future_steps\n",
        "\n",
        "    start_index = start_index + history_size\n",
        "    if end_index is None:\n",
        "        end_index = len(dataset) - target_size\n",
        "\n",
        "    for i in range(start_index, end_index):\n",
        "        indices = getIndices(i,steps)\n",
        "        data.append(dataset[indices])\n",
        "        indiciesL = getFutureIndices(i, future_steps)\n",
        "        labels.append(target[indiciesL])\n",
        "\n",
        "\n",
        "    return np.array(data), np.array(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nd5E-y_o1HY",
        "colab_type": "text"
      },
      "source": [
        "## Single-step LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MseG60kHb2hB",
        "colab_type": "text"
      },
      "source": [
        "### Building network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LYFUp6Mb98n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def singleStepLSTM():\n",
        "    singleStepLSTMModel = keras.Sequential([\n",
        "        layers.LSTM(32, input_shape=(HISTORY_STEPS, len(features))),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "    singleStepLSTMModel.compile(optimizer='adam', loss='mse')\n",
        "    return singleStepLSTMModel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJqrxLV9xCEB",
        "colab_type": "text"
      },
      "source": [
        "### Training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmajAmS4xEs-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataVal = []\n",
        "\n",
        "def trainModel(history_steps, future_step):\n",
        "\n",
        "    models = []\n",
        "    results = []\n",
        "\n",
        "    for x in range(1, FOLDS):\n",
        "\n",
        "        model = singleStepLSTM()\n",
        "\n",
        "        valIndex = fold_locations[x]\n",
        "        \n",
        "        if (x==FOLDS-1):\n",
        "            endIndex = None\n",
        "        else:\n",
        "            endIndex = fold_locations[x+1]\n",
        "\n",
        "        xTrain, yTrain = singleStepDataSplit(dataSet, dataSet[:, 0], 0, valIndex, history_steps, future_step)\n",
        "        xVal, yVal = singleStepDataSplit(dataSet, dataSet[:, 0], valIndex, endIndex, history_steps, future_step)\n",
        "\n",
        "        dataTrain = tf.data.Dataset.from_tensor_slices((xTrain, yTrain))\n",
        "        dataTrain = dataTrain.cache().batch(BATCH_SIZE).repeat()\n",
        "\n",
        "        dataVal = tf.data.Dataset.from_tensor_slices((xVal, yVal))\n",
        "        dataVal = dataVal.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "        print(\"--------------------- Model validated on fold \", \"%d/%d --------------------------\" % (x, FOLDS - 1))\n",
        "\n",
        "        result = model.fit(dataTrain, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                            validation_data=dataVal, validation_steps=50)\n",
        "        \n",
        "        models.append(model)\n",
        "        results.append(result)\n",
        "\n",
        "    return models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STpsQrU84mv0",
        "colab_type": "text"
      },
      "source": [
        "### Single-step tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEgXTi80WseN",
        "colab_type": "code",
        "outputId": "8d4841ed-3e9e-450d-9736-260f1454cefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def singleStepModelTests(model, xTest, yTest):\n",
        "\n",
        "    mses = []\n",
        "    classifications = []\n",
        "\n",
        "    correctDirection = 0\n",
        "\n",
        "    noDatapoints = len(xTest)\n",
        "    noPredictions = 6\n",
        "\n",
        "    mse = model.evaluate(xTest,yTest)\n",
        "\n",
        "    for x in range(noDatapoints):\n",
        "        current = xTest[x][-1]\n",
        "        past = tf.constant([xTest[x]])\n",
        "        prediction = model.predict(past)[0]\n",
        "        future = yTest[x]\n",
        "\n",
        "        if((current > prediction) == (current > future)):\n",
        "                correctDirection = correctDirection + 1\n",
        "\n",
        "    print(\"-------------------------------\")\n",
        "    print(\"MSE: \" + str(round(mse,3)))\n",
        "    directionClass = correctDirection / noDatapoints\n",
        "    print(\"Direction classification: \" + str(round(directionClass,3)))\n",
        "    print(\"-------------------------------\")\n",
        "\n",
        "    mses.append(mse)\n",
        "    classifications.append(directionClass)\n",
        "\n",
        "    return mses, classifications\n",
        "\n",
        "\n",
        "def runModels(models):\n",
        "\n",
        "    modelMses = []\n",
        "    modelClassifications = []\n",
        "\n",
        "    for i in range(1, FOLDS):\n",
        "\n",
        "        valIndex = fold_locations[i]\n",
        "    \n",
        "        if (i==FOLDS-1):\n",
        "            endIndex = None\n",
        "        else:\n",
        "            endIndex = fold_locations[i+1]\n",
        "\n",
        "        xTest, yTest = singleStepDataSplit(dataSet, dataSet[:, 0], valIndex, endIndex, STEPS, FUTURE_STEPS)\n",
        "\n",
        "        mses,classifications = singleStepModelTests(models[i-1], xTest, yTest)\n",
        "\n",
        "        modelMses.append(mses)\n",
        "        modelClassifications.append(classifications)\n",
        "\n",
        "        meanMse = np.mean(modelMses)\n",
        "        meanClass = np.mean(modelClassifications)\n",
        "    \n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(\"Average MSE: \" + str(meanMse))\n",
        "    print(\"Average classification: \" + str(np.mean(classifications)))\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "\n",
        "    return meanMse, meanClass\n",
        "\n",
        "allModels = []\n",
        "allMses = []\n",
        "allClassifications = []\n",
        "\n",
        "def createModelsForAllSteps():\n",
        "\n",
        "    for i in range(1,FUTURE_STEPS+1):\n",
        "        futures_step = i\n",
        "\n",
        "        print(\"&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Month\", \"%d/%d &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\" % (i, FUTURE_STEPS))\n",
        "\n",
        "        models = trainModel(HISTORY_STEPS, i)\n",
        "\n",
        "        modelMses, modelClassifications = runModels(models)\n",
        "\n",
        "        allModels.append(models)\n",
        "        allMses.append(modelMses)\n",
        "        allClassifications.append(modelClassifications)\n",
        "\n",
        "createModelsForAllSteps()\n",
        "\n",
        "def correctMaxTest(finalModels):\n",
        "\n",
        "    valIndex = fold_locations[FOLDS-1]\n",
        "    endIndex = None\n",
        "    correctMax = 0\n",
        "\n",
        "    xTest, yTest = splitData(dataSet, dataSet[:, 0], valIndex, endIndex, HISTORY_STEPS, FUTURE_STEPS)\n",
        "\n",
        "    for datapoint in range(len(xTest)):\n",
        "\n",
        "        past = tf.constant([xTest[datapoint]])\n",
        "        future = yTest[datapoint]\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for j in range(months):\n",
        "\n",
        "            prediction = finalModels[j].predict(past)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        actualMax = np.argmax(future)\n",
        "        predictedMax = np.argmax(predictions)\n",
        "\n",
        "        if(predictedMax == actualMax):\n",
        "            correctMax = correctMax + 1\n",
        "\n",
        "    #print(\"---------------\")\n",
        "    bestMonthClass = correctMax / len(xTest)\n",
        "\n",
        "    #print(\"Correct best month: \" + str(round(bestMonthClass,3)))\n",
        "\n",
        "    return bestMonthClass\n",
        "\n",
        "\n",
        "finalModels = []\n",
        "\n",
        "for i in range(FUTURE_STEPS):\n",
        "    finalModels.append(allModels[i][-1])\n",
        "\n",
        "bestClass = correctMaxTest(finalModels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Month 1/6 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "--------------------- Model validated on fold  1/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.1073 - val_loss: 0.1466\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0561 - val_loss: 0.1374\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0425 - val_loss: 0.1332\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0316 - val_loss: 0.1389\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0258 - val_loss: 0.1270\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0223 - val_loss: 0.1159\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0195 - val_loss: 0.1041\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0180 - val_loss: 0.0905\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.0158 - val_loss: 0.0832\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0157 - val_loss: 0.0766\n",
            "--------------------- Model validated on fold  2/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 0.3122 - val_loss: 0.4162\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1406 - val_loss: 0.3303\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1132 - val_loss: 0.2686\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0923 - val_loss: 0.2199\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0773 - val_loss: 0.1806\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0660 - val_loss: 0.1491\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0542 - val_loss: 0.1232\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0416 - val_loss: 0.1018\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0331 - val_loss: 0.0861\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0279 - val_loss: 0.0738\n",
            "--------------------- Model validated on fold  3/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.4486 - val_loss: 0.1819\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1475 - val_loss: 0.1854\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1442 - val_loss: 0.1258\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0783 - val_loss: 0.1095\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0796 - val_loss: 0.0858\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0613 - val_loss: 0.0839\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0694 - val_loss: 0.0735\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0599 - val_loss: 0.0732\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0523 - val_loss: 0.0692\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0506 - val_loss: 0.0631\n",
            "--------------------- Model validated on fold  4/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.2299 - val_loss: 0.0410\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.1361 - val_loss: 0.0269\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0927 - val_loss: 0.0120\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0732 - val_loss: 0.0239\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0534 - val_loss: 0.0497\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0466 - val_loss: 0.0257\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0565 - val_loss: 0.0331\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0515 - val_loss: 0.0459\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0434 - val_loss: 0.0482\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0420 - val_loss: 0.0281\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.4107\n",
            "-------------------------------\n",
            "MSE: 0.411\n",
            "Direction classification: 0.497\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1047\n",
            "-------------------------------\n",
            "MSE: 0.105\n",
            "Direction classification: 0.609\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.3278\n",
            "-------------------------------\n",
            "MSE: 0.328\n",
            "Direction classification: 0.245\n",
            "-------------------------------\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0277\n",
            "-------------------------------\n",
            "MSE: 0.028\n",
            "Direction classification: 0.546\n",
            "-------------------------------\n",
            "-------------------------------------------------------------\n",
            "Average MSE: 0.21773412311449647\n",
            "Average classification: 0.5461741424802111\n",
            "-------------------------------------------------------------\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Month 2/6 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "--------------------- Model validated on fold  1/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.1280 - val_loss: 0.2383\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0621 - val_loss: 0.2460\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0550 - val_loss: 0.2451\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0487 - val_loss: 0.2446\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0414 - val_loss: 0.2500\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0347 - val_loss: 0.2509\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0291 - val_loss: 0.2416\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0268 - val_loss: 0.2308\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0230 - val_loss: 0.2246\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0243 - val_loss: 0.2030\n",
            "--------------------- Model validated on fold  2/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.3028 - val_loss: 0.4209\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1551 - val_loss: 0.3418\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1173 - val_loss: 0.2957\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0962 - val_loss: 0.2581\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0831 - val_loss: 0.2270\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0751 - val_loss: 0.2012\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0640 - val_loss: 0.1793\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0552 - val_loss: 0.1606\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0495 - val_loss: 0.1452\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0460 - val_loss: 0.1309\n",
            "--------------------- Model validated on fold  3/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.2741 - val_loss: 0.2303\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1273 - val_loss: 0.1806\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1481 - val_loss: 0.1294\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0881 - val_loss: 0.1300\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0940 - val_loss: 0.1000\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0755 - val_loss: 0.1016\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0937 - val_loss: 0.0911\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0756 - val_loss: 0.0998\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0706 - val_loss: 0.0925\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0707 - val_loss: 0.0908\n",
            "--------------------- Model validated on fold  4/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.2944 - val_loss: 0.0340\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1844 - val_loss: 0.0344\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1375 - val_loss: 0.0166\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1109 - val_loss: 0.0334\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0904 - val_loss: 0.0582\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0672 - val_loss: 0.0351\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0925 - val_loss: 0.0499\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0862 - val_loss: 0.0675\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 10ms/step - loss: 0.0746 - val_loss: 0.0661\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0752 - val_loss: 0.0384\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.5359\n",
            "-------------------------------\n",
            "MSE: 0.536\n",
            "Direction classification: 0.576\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 0.1422\n",
            "-------------------------------\n",
            "MSE: 0.142\n",
            "Direction classification: 0.566\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3054\n",
            "-------------------------------\n",
            "MSE: 0.305\n",
            "Direction classification: 0.286\n",
            "-------------------------------\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0300\n",
            "-------------------------------\n",
            "MSE: 0.03\n",
            "Direction classification: 0.546\n",
            "-------------------------------\n",
            "-------------------------------------------------------------\n",
            "Average MSE: 0.25337497936561704\n",
            "Average classification: 0.5461741424802111\n",
            "-------------------------------------------------------------\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Month 3/6 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "--------------------- Model validated on fold  1/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.1606 - val_loss: 0.3200\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0603 - val_loss: 0.3452\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0528 - val_loss: 0.3597\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0490 - val_loss: 0.3762\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0439 - val_loss: 0.3935\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0389 - val_loss: 0.3967\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0338 - val_loss: 0.3834\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0301 - val_loss: 0.3604\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0242 - val_loss: 0.3485\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0231 - val_loss: 0.3374\n",
            "--------------------- Model validated on fold  2/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.2379 - val_loss: 0.3475\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1716 - val_loss: 0.2862\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1433 - val_loss: 0.2444\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1287 - val_loss: 0.2097\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1188 - val_loss: 0.1811\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1133 - val_loss: 0.1573\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1026 - val_loss: 0.1374\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0915 - val_loss: 0.1214\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0830 - val_loss: 0.1088\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0766 - val_loss: 0.0958\n",
            "--------------------- Model validated on fold  3/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.2542 - val_loss: 0.2809\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1549 - val_loss: 0.2337\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1867 - val_loss: 0.1790\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1110 - val_loss: 0.1710\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1209 - val_loss: 0.1346\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0994 - val_loss: 0.1417\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1280 - val_loss: 0.1292\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0971 - val_loss: 0.1313\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0938 - val_loss: 0.1163\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0962 - val_loss: 0.1273\n",
            "--------------------- Model validated on fold  4/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.2868 - val_loss: 0.0337\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.2074 - val_loss: 0.0331\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1623 - val_loss: 0.0175\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1404 - val_loss: 0.0350\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1208 - val_loss: 0.0675\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0832 - val_loss: 0.0441\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1224 - val_loss: 0.0715\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1154 - val_loss: 0.1005\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1016 - val_loss: 0.0938\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1023 - val_loss: 0.0458\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.6294\n",
            "-------------------------------\n",
            "MSE: 0.629\n",
            "Direction classification: 0.615\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1116\n",
            "-------------------------------\n",
            "MSE: 0.112\n",
            "Direction classification: 0.582\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.3044\n",
            "-------------------------------\n",
            "MSE: 0.304\n",
            "Direction classification: 0.284\n",
            "-------------------------------\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0389\n",
            "-------------------------------\n",
            "MSE: 0.039\n",
            "Direction classification: 0.536\n",
            "-------------------------------\n",
            "-------------------------------------------------------------\n",
            "Average MSE: 0.27107624523341656\n",
            "Average classification: 0.5356200527704486\n",
            "-------------------------------------------------------------\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Month 4/6 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "--------------------- Model validated on fold  1/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.2401 - val_loss: 0.4440\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0506 - val_loss: 0.4730\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0426 - val_loss: 0.4902\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0401 - val_loss: 0.5231\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0372 - val_loss: 0.5806\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0344 - val_loss: 0.6078\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0319 - val_loss: 0.6109\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0295 - val_loss: 0.6021\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0254 - val_loss: 0.6077\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0227 - val_loss: 0.6027\n",
            "--------------------- Model validated on fold  2/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.2221 - val_loss: 0.1981\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1797 - val_loss: 0.1764\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1562 - val_loss: 0.1634\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1439 - val_loss: 0.1534\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1374 - val_loss: 0.1453\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1320 - val_loss: 0.1386\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1217 - val_loss: 0.1330\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1129 - val_loss: 0.1294\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1068 - val_loss: 0.1269\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1032 - val_loss: 0.1232\n",
            "--------------------- Model validated on fold  3/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.2907 - val_loss: 0.3021\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1885 - val_loss: 0.2446\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2148 - val_loss: 0.2176\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1346 - val_loss: 0.2346\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1498 - val_loss: 0.1817\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1227 - val_loss: 0.1900\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1603 - val_loss: 0.1817\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1173 - val_loss: 0.2040\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1180 - val_loss: 0.1882\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1168 - val_loss: 0.1942\n",
            "--------------------- Model validated on fold  4/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.4059 - val_loss: 0.0549\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2727 - val_loss: 0.0550\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2150 - val_loss: 0.0255\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1884 - val_loss: 0.0682\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1673 - val_loss: 0.0957\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1121 - val_loss: 0.0633\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1640 - val_loss: 0.0909\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.1553 - val_loss: 0.1187\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1371 - val_loss: 0.1151\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1372 - val_loss: 0.0627\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.8858\n",
            "-------------------------------\n",
            "MSE: 0.886\n",
            "Direction classification: 0.617\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1331\n",
            "-------------------------------\n",
            "MSE: 0.133\n",
            "Direction classification: 0.566\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.3017\n",
            "-------------------------------\n",
            "MSE: 0.302\n",
            "Direction classification: 0.367\n",
            "-------------------------------\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0542\n",
            "-------------------------------\n",
            "MSE: 0.054\n",
            "Direction classification: 0.536\n",
            "-------------------------------\n",
            "-------------------------------------------------------------\n",
            "Average MSE: 0.3437245190143585\n",
            "Average classification: 0.5356200527704486\n",
            "-------------------------------------------------------------\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Month 5/6 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "--------------------- Model validated on fold  1/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.1293 - val_loss: 0.4143\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0488 - val_loss: 0.5117\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0284 - val_loss: 0.5859\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0247 - val_loss: 0.6046\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0235 - val_loss: 0.6436\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0222 - val_loss: 0.6673\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0210 - val_loss: 0.6682\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0189 - val_loss: 0.6506\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0160 - val_loss: 0.6329\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.0138 - val_loss: 0.6204\n",
            "--------------------- Model validated on fold  2/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.3439 - val_loss: 0.3969\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2163 - val_loss: 0.3467\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1999 - val_loss: 0.3088\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1887 - val_loss: 0.2766\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1840 - val_loss: 0.2471\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1773 - val_loss: 0.2190\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1663 - val_loss: 0.1914\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1572 - val_loss: 0.1645\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1514 - val_loss: 0.1381\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1477 - val_loss: 0.1125\n",
            "--------------------- Model validated on fold  3/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.2708 - val_loss: 0.3383\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2259 - val_loss: 0.3172\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2641 - val_loss: 0.2483\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1656 - val_loss: 0.2698\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1864 - val_loss: 0.2031\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1562 - val_loss: 0.2222\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2031 - val_loss: 0.2080\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1440 - val_loss: 0.2306\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1495 - val_loss: 0.2025\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1456 - val_loss: 0.2215\n",
            "--------------------- Model validated on fold  4/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 0.3823 - val_loss: 0.0444\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2898 - val_loss: 0.0446\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2241 - val_loss: 0.0215\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2127 - val_loss: 0.0542\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1964 - val_loss: 0.0922\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1253 - val_loss: 0.0678\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1908 - val_loss: 0.1012\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1827 - val_loss: 0.1288\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1654 - val_loss: 0.1185\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1669 - val_loss: 0.0639\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.8195\n",
            "-------------------------------\n",
            "MSE: 0.819\n",
            "Direction classification: 0.576\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.1167\n",
            "-------------------------------\n",
            "MSE: 0.117\n",
            "Direction classification: 0.566\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2730\n",
            "-------------------------------\n",
            "MSE: 0.273\n",
            "Direction classification: 0.493\n",
            "-------------------------------\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0569\n",
            "-------------------------------\n",
            "MSE: 0.057\n",
            "Direction classification: 0.536\n",
            "-------------------------------\n",
            "-------------------------------------------------------------\n",
            "Average MSE: 0.31653143372386694\n",
            "Average classification: 0.5356200527704486\n",
            "-------------------------------------------------------------\n",
            "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&& Month 6/6 &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
            "--------------------- Model validated on fold  1/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 0.1363 - val_loss: 0.5697\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0381 - val_loss: 0.7072\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0202 - val_loss: 0.7536\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0172 - val_loss: 0.7477\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0166 - val_loss: 0.7771\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0152 - val_loss: 0.8004\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0147 - val_loss: 0.7889\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0134 - val_loss: 0.7687\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.0115 - val_loss: 0.7488\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.0105 - val_loss: 0.7504\n",
            "--------------------- Model validated on fold  2/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.3584 - val_loss: 0.2721\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2846 - val_loss: 0.2193\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2557 - val_loss: 0.1817\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2399 - val_loss: 0.1527\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 0.2318 - val_loss: 0.1293\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2224 - val_loss: 0.1106\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2071 - val_loss: 0.0974\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1976 - val_loss: 0.0908\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1908 - val_loss: 0.0932\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1867 - val_loss: 0.1067\n",
            "--------------------- Model validated on fold  3/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.3882 - val_loss: 0.3282\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2584 - val_loss: 0.3263\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.3010 - val_loss: 0.3093\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1844 - val_loss: 0.3457\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2062 - val_loss: 0.2711\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1746 - val_loss: 0.2901\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2317 - val_loss: 0.2737\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1556 - val_loss: 0.3093\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1662 - val_loss: 0.2643\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1551 - val_loss: 0.2802\n",
            "--------------------- Model validated on fold  4/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.4578 - val_loss: 0.1067\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3110 - val_loss: 0.0534\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2632 - val_loss: 0.0303\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2498 - val_loss: 0.0713\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2329 - val_loss: 0.1003\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1439 - val_loss: 0.0699\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2230 - val_loss: 0.0993\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2151 - val_loss: 0.1254\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1949 - val_loss: 0.1218\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.1991 - val_loss: 0.0659\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.8200\n",
            "-------------------------------\n",
            "MSE: 0.82\n",
            "Direction classification: 0.58\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 0.1085\n",
            "-------------------------------\n",
            "MSE: 0.108\n",
            "Direction classification: 0.391\n",
            "-------------------------------\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 0.2816\n",
            "-------------------------------\n",
            "MSE: 0.282\n",
            "Direction classification: 0.481\n",
            "-------------------------------\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.0682\n",
            "-------------------------------\n",
            "MSE: 0.068\n",
            "Direction classification: 0.536\n",
            "-------------------------------\n",
            "-------------------------------------------------------------\n",
            "Average MSE: 0.31957047060132027\n",
            "Average classification: 0.5356200527704486\n",
            "-------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwgG0U6-gUrZ",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DGg4vGuUmRd",
        "colab_type": "code",
        "outputId": "50b9380f-70b2-4d2c-8162-32d9c81d1733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "def printResults(mses,directionClass,bestClass):\n",
        "\n",
        "    for i in range(FUTURE_STEPS):\n",
        "\n",
        "        print(\"-----------\")\n",
        "        print(\"Month \" + str((i+1)))\n",
        "        print(\"MSE: \" + str(mses[i]))\n",
        "        print(\"Dir: \" + str(directionClass[i]))\n",
        "\n",
        "    print(\"---------\")\n",
        "    print(\"Bes: \" + str(bestClass))\n",
        "\n",
        "printResults(allMses, allClassifications, bestClass)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "Month 1\n",
            "MSE: 0.21773412311449647\n",
            "Dir: 0.4743147387758713\n",
            "-----------\n",
            "Month 2\n",
            "MSE: 0.25337497936561704\n",
            "Dir: 0.4935455080066406\n",
            "-----------\n",
            "Month 3\n",
            "MSE: 0.27107624523341656\n",
            "Dir: 0.5042205950466556\n",
            "-----------\n",
            "Month 4\n",
            "MSE: 0.3437245190143585\n",
            "Dir: 0.5214789776896536\n",
            "-----------\n",
            "Month 5\n",
            "MSE: 0.31653143372386694\n",
            "Dir: 0.542682133508194\n",
            "-----------\n",
            "Month 6\n",
            "MSE: 0.31957047060132027\n",
            "Dir: 0.4968241453425135\n",
            "---------\n",
            "Bes: 0.1424802110817942\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uwx3VvAnnRN",
        "colab_type": "text"
      },
      "source": [
        "### Prediction visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TYzy6YXnpjs",
        "colab_type": "code",
        "outputId": "d1b991b6-cafe-4a60-ee57-7e401307f971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_time_steps(length,steps):\n",
        "    return list(np.arange(-length, 0,step=steps))\n",
        "\n",
        "def show_plot(plot_data, delta, title):\n",
        "    labels = ['History', 'True Future', 'Model Prediction']\n",
        "    marker = ['.-', 'rx', 'go']\n",
        "    time_steps = create_time_steps(TIME_LAGS,STEP)\n",
        "\n",
        "    if delta:\n",
        "        future = delta\n",
        "    else:\n",
        "        future = 0\n",
        "\n",
        "    plt.title(title)\n",
        "    for i, x in enumerate(plot_data):\n",
        "        if i:\n",
        "            plt.plot(future, plot_data[i], marker[i], markersize=10,\n",
        "                    label=labels[i])\n",
        "        else:\n",
        "            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n",
        "\n",
        "    plt.legend()\n",
        "    plt.xlim([time_steps[0], (future+5)*2])\n",
        "    plt.xlabel('Time-Step')\n",
        "    return plt\n",
        "\n",
        "\n",
        "for x, y in dataVal.take(1):\n",
        "    plot = show_plot([x[0][:, 0].numpy(), y[0].numpy(),\n",
        "                        model.predict(x)[0]], PREDICTION_HORIZON,\n",
        "                    'Single Step Prediction')\n",
        "    print(model.predict(x)[0])\n",
        "    plot.show()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-f85ae4b450d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataVal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     plot = show_plot([x[0][:, 0].numpy(), y[0].numpy(),\n\u001b[1;32m     33\u001b[0m                         model.predict(x)[0]], PREDICTION_HORIZON,\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'take'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1syGFxznuVLK",
        "colab_type": "text"
      },
      "source": [
        "## Multi-step LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OHJMXYsuZWp",
        "colab_type": "text"
      },
      "source": [
        "### Building network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0P_sbFRuhMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multiStepLSTM():\n",
        "    multiStepLSTMModel = keras.Sequential([\n",
        "        layers.LSTM(units=32, return_sequences=True, input_shape = (HISTORY_STEPS, len(features))),\n",
        "        layers.LSTM(16, activation='relu'),\n",
        "        layers.Dense(FUTURE_STEPS)\n",
        "    ])\n",
        "\n",
        "    multiStepLSTMModel.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='mse')\n",
        "    return multiStepLSTMModel\n",
        "\n",
        "multiStepModel = multiStepLSTM()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_IwcoGcu3mP",
        "colab_type": "text"
      },
      "source": [
        "### Training models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4UAjl2pn0eT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd41ee0a-15c8-4966-f464-0f239817e55e"
      },
      "source": [
        "models = []\n",
        "results = []\n",
        "dataTrainMulti = []\n",
        "\n",
        "for x in range(1, FOLDS):\n",
        "\n",
        "    valIndex = fold_locations[x]\n",
        "\n",
        "    multiStepModel = multiStepLSTM()\n",
        "    \n",
        "    if (x==FOLDS-1):\n",
        "        endIndex = None\n",
        "    else:\n",
        "        endIndex = fold_locations[x+1]\n",
        "\n",
        "    xTrainMulti, yTrainMulti = splitData(dataSet, dataSet[:, 0], 0, valIndex, HISTORY_STEPS, FUTURE_STEPS)\n",
        "    xValMulti, yValMulti = splitData(dataSet, dataSet[:, 0], valIndex, None, HISTORY_STEPS, FUTURE_STEPS)\n",
        "\n",
        "    dataTrainMulti = tf.data.Dataset.from_tensor_slices((xTrainMulti, yTrainMulti))\n",
        "    dataTrainMulti = dataTrainMulti.cache().batch(BATCH_SIZE).repeat()\n",
        "\n",
        "    dataValMulti = tf.data.Dataset.from_tensor_slices((xValMulti, yValMulti))\n",
        "    dataValMulti = dataValMulti.batch(BATCH_SIZE).repeat()\n",
        "\n",
        "    print(\"--------------------- Model\", \"%d/%d --------------------------\" % (x, FOLDS - 1))\n",
        "\n",
        "    result = multiStepModel.fit(dataTrainMulti, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL,\n",
        "                        validation_data=dataValMulti, validation_steps=50)\n",
        "    \n",
        "    models.append(multiStepModel)\n",
        "    results.append(result)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------- Model 1/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 0.3138 - val_loss: 0.9895\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0473 - val_loss: 1.1463\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0378 - val_loss: 1.1037\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0364 - val_loss: 1.0269\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0374 - val_loss: 1.0294\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0384 - val_loss: 1.1002\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0383 - val_loss: 1.2323\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0384 - val_loss: 1.2804\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0365 - val_loss: 1.1682\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0383 - val_loss: 1.2701\n",
            "--------------------- Model 2/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 3s 27ms/step - loss: 0.2750 - val_loss: 0.4466\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0930 - val_loss: 0.4401\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 2s 23ms/step - loss: 0.0956 - val_loss: 0.4335\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 3s 33ms/step - loss: 0.0948 - val_loss: 0.4282\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 4s 35ms/step - loss: 0.0808 - val_loss: 0.4215\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0790 - val_loss: 0.4163\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0764 - val_loss: 0.4121\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0653 - val_loss: 0.4038\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0582 - val_loss: 0.3929\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0540 - val_loss: 0.3822\n",
            "--------------------- Model 3/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 3s 28ms/step - loss: 0.3228 - val_loss: 0.4235\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.2976 - val_loss: 0.2899\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.1057 - val_loss: 0.1794\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0844 - val_loss: 0.1777\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.1393 - val_loss: 0.2217\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0656 - val_loss: 0.2416\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0642 - val_loss: 0.1965\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 2s 24ms/step - loss: 0.0586 - val_loss: 0.1976\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0592 - val_loss: 0.1978\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0404 - val_loss: 0.2239\n",
            "--------------------- Model 4/4 --------------------------\n",
            "Epoch 1/10\n",
            "100/100 [==============================] - 3s 26ms/step - loss: 0.2653 - val_loss: 0.2879\n",
            "Epoch 2/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.1525 - val_loss: 1.5454\n",
            "Epoch 3/10\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 0.1056 - val_loss: 0.0459\n",
            "Epoch 4/10\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 0.0801 - val_loss: 0.2585\n",
            "Epoch 5/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0756 - val_loss: 0.9991\n",
            "Epoch 6/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0635 - val_loss: 0.6459\n",
            "Epoch 7/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0726 - val_loss: 0.7428\n",
            "Epoch 8/10\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.0701 - val_loss: 0.6868\n",
            "Epoch 9/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0663 - val_loss: 0.7694\n",
            "Epoch 10/10\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.0557 - val_loss: 0.6284\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2Uaxl7U4rEH",
        "colab_type": "text"
      },
      "source": [
        "### Multi-step tests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CxChDHwvxdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7da19304-6b14-4a68-c9fb-d6e6ef3ca362"
      },
      "source": [
        "def multiStepModelTests(model, xTest, yTest):\n",
        "\n",
        "    correctDirection = [0,0,0,0,0,0]\n",
        "    totalSquaredError = [0,0,0,0,0,0]\n",
        "    correctMax = 0\n",
        "    correctMaxRelaxed = 0\n",
        "\n",
        "    noDatapoints = len(xTest)\n",
        "    noPredictions = len(totalSquaredError)\n",
        "\n",
        "    for x in range(noDatapoints):\n",
        "        current = xTest[x][-1]\n",
        "        past = tf.constant([xTest[x]])\n",
        "        predictions = model.predict(past)[0]\n",
        "        future = yTest[x]\n",
        "\n",
        "        for y in range(noPredictions):\n",
        "            prediction = predictions[y]\n",
        "            actual = future[y]\n",
        "\n",
        "            squaredDifference = abs(prediction - actual) ** 2\n",
        "            totalSquaredError[y] = totalSquaredError[y] + squaredDifference\n",
        "\n",
        "            if ((current > prediction) == (current > actual)):\n",
        "                correctDirection[y] = correctDirection[y] + 1\n",
        "\n",
        "    mses = []\n",
        "    classifications = []\n",
        "\n",
        "\n",
        "    print(\"---------------\")\n",
        "    for x in range(noPredictions):\n",
        "        mse = totalSquaredError[x] / noDatapoints\n",
        "        mses.append(mse)\n",
        "        print(\"MSE (Month \" + str(x+1) + \"): \" + str(round(mse,3)))\n",
        "\n",
        "    averageMSE = (sum(totalSquaredError) / noDatapoints) / noPredictions\n",
        "    print(\"Average MSE: \" + str(round(averageMSE,3)))\n",
        "\n",
        "\n",
        "    print(\"---------------\")\n",
        "    for x in range(noPredictions):\n",
        "        percentInterval = correctDirection[x] / noDatapoints\n",
        "        classifications.append(percentInterval)\n",
        "        print(\"Direction (Month \" + str(x+1) + \"): \" + str(round(percentInterval,3)))\n",
        "\n",
        "    directionClass = (sum(correctDirection) / noDatapoints) / noPredictions\n",
        "    print(\"Average classification: \" + str(round(directionClass,3)))\n",
        "\n",
        "    return mses, classifications\n",
        "\n",
        "\n",
        "def runModels(models):\n",
        "\n",
        "    modelMses = []\n",
        "    modelClassifications = []\n",
        "\n",
        "    for i in range(1, FOLDS):\n",
        "\n",
        "        valIndex = fold_locations[i]\n",
        "    \n",
        "        if (i==FOLDS-1):\n",
        "            endIndex = None\n",
        "        else:\n",
        "            endIndex = fold_locations[i+1]\n",
        "\n",
        "        xTestMulti, yTestMulti = splitData(dataSet, dataSet[:, 0], valIndex, endIndex, HISTORY_STEPS, FUTURE_STEPS)\n",
        "\n",
        "        mses, classifications = multiStepModelTests(models[i-1], xTestMulti, yTestMulti)\n",
        "\n",
        "        modelMses.append(mses)\n",
        "        modelClassifications.append(classifications)\n",
        "\n",
        "    monthAverageMses = []\n",
        "    monthAverageClass = []\n",
        "\n",
        "    for i in range(FUTURE_STEPS):\n",
        "\n",
        "        totalMse = 0\n",
        "        totalClass = 0\n",
        "\n",
        "        for j in range(len(modelMses)):\n",
        "            totalMse = totalMse + modelMses[j][i]\n",
        "            totalClass = totalClass + modelClassifications[j][i]\n",
        "\n",
        "        meanMse = totalMse / len(modelMses)\n",
        "        meanClass = totalClass / len(modelMses)\n",
        "\n",
        "        monthAverageMses.append(meanMse)\n",
        "        monthAverageClass.append(meanClass)\n",
        "\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "    print(\"Average MSE: \" + str(meanMse))\n",
        "    print(\"Average classification: \" + str(np.mean(classifications)))\n",
        "    print(\"-------------------------------------------------------------\")\n",
        "\n",
        "    return monthAverageMses, monthAverageClass\n",
        "\n",
        "def correctMaxTest(finalModel):\n",
        "\n",
        "    valIndex = fold_locations[4]\n",
        "    endIndex = None\n",
        "    correctMax = 0\n",
        "\n",
        "    xTestMulti, yTestMulti = splitData(dataSet, dataSet[:, 0], valIndex, endIndex, STEPS, FUTURE_STEPS)\n",
        "\n",
        "    for datapoint in range(len(xTestMulti)):\n",
        "\n",
        "        past = tf.constant([xTestMulti[datapoint]])\n",
        "        future = yTestMulti[datapoint]\n",
        "\n",
        "        predictions = finalModel.predict(past)[0]\n",
        "\n",
        "        actualMax = np.argmax(future)\n",
        "        predictedMax = np.argmax(predictions)\n",
        "\n",
        "        if(predictedMax == actualMax):\n",
        "            correctMax = correctMax + 1\n",
        "\n",
        "    bestMonthClass = correctMax / len(xTestMulti)\n",
        "\n",
        "    print(\"Correct best month: \" + str(round(bestMonthClass,3)))\n",
        "\n",
        "    return bestMonthClass\n",
        "\n",
        "monthAverageMses, monthAverageClass = runModels(models)\n",
        "bestClass = correctMaxTest(models[-1])"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------\n",
            "MSE (Month 1): 0.435\n",
            "MSE (Month 2): 0.413\n",
            "MSE (Month 3): 0.461\n",
            "MSE (Month 4): 0.782\n",
            "MSE (Month 5): 0.689\n",
            "MSE (Month 6): 0.776\n",
            "Average MSE: 0.593\n",
            "---------------\n",
            "Direction (Month 1): 0.428\n",
            "Direction (Month 2): 0.412\n",
            "Direction (Month 3): 0.381\n",
            "Direction (Month 4): 0.513\n",
            "Direction (Month 5): 0.554\n",
            "Direction (Month 6): 0.611\n",
            "Average classification: 0.483\n",
            "---------------\n",
            "MSE (Month 1): 0.659\n",
            "MSE (Month 2): 0.851\n",
            "MSE (Month 3): 0.512\n",
            "MSE (Month 4): 0.686\n",
            "MSE (Month 5): 0.498\n",
            "MSE (Month 6): 0.483\n",
            "Average MSE: 0.615\n",
            "---------------\n",
            "Direction (Month 1): 0.607\n",
            "Direction (Month 2): 0.609\n",
            "Direction (Month 3): 0.659\n",
            "Direction (Month 4): 0.633\n",
            "Direction (Month 5): 0.582\n",
            "Direction (Month 6): 0.568\n",
            "Average classification: 0.61\n",
            "---------------\n",
            "MSE (Month 1): 0.166\n",
            "MSE (Month 2): 0.177\n",
            "MSE (Month 3): 0.319\n",
            "MSE (Month 4): 0.193\n",
            "MSE (Month 5): 0.195\n",
            "MSE (Month 6): 0.22\n",
            "Average MSE: 0.212\n",
            "---------------\n",
            "Direction (Month 1): 0.564\n",
            "Direction (Month 2): 0.422\n",
            "Direction (Month 3): 0.446\n",
            "Direction (Month 4): 0.771\n",
            "Direction (Month 5): 0.832\n",
            "Direction (Month 6): 0.799\n",
            "Average classification: 0.639\n",
            "---------------\n",
            "MSE (Month 1): 0.441\n",
            "MSE (Month 2): 0.51\n",
            "MSE (Month 3): 0.512\n",
            "MSE (Month 4): 0.532\n",
            "MSE (Month 5): 0.811\n",
            "MSE (Month 6): 0.952\n",
            "Average MSE: 0.627\n",
            "---------------\n",
            "Direction (Month 1): 0.559\n",
            "Direction (Month 2): 0.639\n",
            "Direction (Month 3): 0.681\n",
            "Direction (Month 4): 0.652\n",
            "Direction (Month 5): 0.538\n",
            "Direction (Month 6): 0.52\n",
            "Average classification: 0.598\n",
            "-------------------------------------------------------------\n",
            "Average MSE: 0.60780673028778\n",
            "Average classification: 0.5980650835532102\n",
            "-------------------------------------------------------------\n",
            "Correct best month: 0.214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNSYYPtFgP4i",
        "colab_type": "text"
      },
      "source": [
        "### Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R2SBad6gSMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "c5a58cc9-33eb-4d97-9917-7c68ce5fea69"
      },
      "source": [
        "def printResults(monthAverageMses,monthAverageClass,bestClass):\n",
        "\n",
        "    for i in range(len(monthAverageMses)):\n",
        "\n",
        "        print(\"-----------\")\n",
        "        print(\"Month \" + str((i+1)))\n",
        "        print(\"MSE: \" + str(monthAverageMses[i]))\n",
        "        print(\"Dir: \" + str(monthAverageClass[i]))\n",
        "\n",
        "    print(\"---------\")\n",
        "    print(\"Bes: \" + str(bestClass))\n",
        "\n",
        "printResults(monthAverageMses, monthAverageClass, bestClass)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------\n",
            "Month 1\n",
            "MSE: 0.4255993535475884\n",
            "Dir: 0.539743069324965\n",
            "-----------\n",
            "Month 2\n",
            "MSE: 0.48780561998613103\n",
            "Dir: 0.5205773524222884\n",
            "-----------\n",
            "Month 3\n",
            "MSE: 0.45124988191689436\n",
            "Dir: 0.5414864717178498\n",
            "-----------\n",
            "Month 4\n",
            "MSE: 0.5480984739718459\n",
            "Dir: 0.6422187007228615\n",
            "-----------\n",
            "Month 5\n",
            "MSE: 0.5483305819754664\n",
            "Dir: 0.6266750974483875\n",
            "-----------\n",
            "Month 6\n",
            "MSE: 0.60780673028778\n",
            "Dir: 0.6245231664350804\n",
            "---------\n",
            "Bes: 0.21372031662269128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tii3SZcOw1qb",
        "colab_type": "text"
      },
      "source": [
        "### Prediction visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gR0uE5bfw5uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a47cdc9c-7096-45d3-e09a-a4fba85852e2"
      },
      "source": [
        "def multi_step_plot(history, true_future, prediction):\n",
        "  plt.figure(figsize=(12, 6))\n",
        "  num_in = create_time_steps(TIME_LAGS,STEP)\n",
        "  num_out = len(true_future) * FUTURE_STEP\n",
        "\n",
        "  plt.plot(num_in, np.array(history[:, 0]), label='History')\n",
        "  plt.plot(np.arange(num_out, step=FUTURE_STEP), np.array(true_future), 'bo',\n",
        "           label='True Future')\n",
        "  if prediction.any():\n",
        "    plt.plot(np.arange(num_out,step=FUTURE_STEP), np.array(prediction), 'ro',\n",
        "             label='Predicted Future')\n",
        "  plt.legend(loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "model = models[-1]\n",
        "\n",
        "for x, y in dataTrainMulti.take(1):\n",
        "\n",
        "    print(x)\n",
        "\n",
        "    #print((multiStepModel.predict(x)[0]).index(max(multiStepModel.predict(x)[0])))\n",
        "    print(model.predict(x)[0])\n",
        "    multi_step_plot(x[0], y[0], model.predict(x)[0])\n"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[[1.59906008]\n",
            "  [1.6374907 ]\n",
            "  [1.75694842]\n",
            "  [1.83452696]\n",
            "  [2.02618348]\n",
            "  [2.06806265]\n",
            "  [1.86702608]\n",
            "  [1.77041155]\n",
            "  [1.75576211]\n",
            "  [1.85069375]\n",
            "  [1.7999863 ]\n",
            "  [1.89312469]\n",
            "  [1.95519856]\n",
            "  [1.91952678]\n",
            "  [1.94432874]\n",
            "  [1.76652159]\n",
            "  [1.49643128]\n",
            "  [1.53905534]\n",
            "  [1.60449499]\n",
            "  [1.67600409]]\n",
            "\n",
            " [[1.595915  ]\n",
            "  [1.63864941]\n",
            "  [1.76894936]\n",
            "  [1.8332303 ]\n",
            "  [2.02872161]\n",
            "  [2.07266991]\n",
            "  [1.84956263]\n",
            "  [1.77187373]\n",
            "  [1.75725189]\n",
            "  [1.85441819]\n",
            "  [1.80053807]\n",
            "  [1.89386958]\n",
            "  [1.95845399]\n",
            "  [1.91538852]\n",
            "  [1.94339073]\n",
            "  [1.75402405]\n",
            "  [1.48564422]\n",
            "  [1.55215982]\n",
            "  [1.60035673]\n",
            "  [1.68058376]]\n",
            "\n",
            " [[1.59061803]\n",
            "  [1.64369809]\n",
            "  [1.77675688]\n",
            "  [1.83607191]\n",
            "  [2.03236328]\n",
            "  [2.07120773]\n",
            "  [1.83618226]\n",
            "  [1.7726738 ]\n",
            "  [1.7610315 ]\n",
            "  [1.85681838]\n",
            "  [1.80197266]\n",
            "  [1.89348334]\n",
            "  [1.96333714]\n",
            "  [1.91249174]\n",
            "  [1.94137678]\n",
            "  [1.73959532]\n",
            "  [1.47863677]\n",
            "  [1.56451942]\n",
            "  [1.59533564]\n",
            "  [1.68472202]]\n",
            "\n",
            " [[1.58769366]\n",
            "  [1.65100901]\n",
            "  [1.78621969]\n",
            "  [1.83816863]\n",
            "  [2.03594977]\n",
            "  [2.0696076 ]\n",
            "  [1.82338125]\n",
            "  [1.77212203]\n",
            "  [1.7676803 ]\n",
            "  [1.85745291]\n",
            "  [1.80415214]\n",
            "  [1.89522141]\n",
            "  [1.96833064]\n",
            "  [1.91166409]\n",
            "  [1.93831447]\n",
            "  [1.72469759]\n",
            "  [1.47502269]\n",
            "  [1.57315458]\n",
            "  [1.59100427]\n",
            "  [1.69043281]]\n",
            "\n",
            " [[1.59252163]\n",
            "  [1.65572662]\n",
            "  [1.79634463]\n",
            "  [1.84153441]\n",
            "  [2.03796372]\n",
            "  [2.06916619]\n",
            "  [1.80980776]\n",
            "  [1.77341868]\n",
            "  [1.77328074]\n",
            "  [1.86073593]\n",
            "  [1.80666269]\n",
            "  [1.89759401]\n",
            "  [1.97351725]\n",
            "  [1.90929149]\n",
            "  [1.93478316]\n",
            "  [1.7106551 ]\n",
            "  [1.47336739]\n",
            "  [1.58336228]\n",
            "  [1.58573489]\n",
            "  [1.69633673]]\n",
            "\n",
            " [[1.59814966]\n",
            "  [1.65705087]\n",
            "  [1.80222096]\n",
            "  [1.85050063]\n",
            "  [2.0420468 ]\n",
            "  [2.06309674]\n",
            "  [1.79491004]\n",
            "  [1.77885359]\n",
            "  [1.77546022]\n",
            "  [1.86286023]\n",
            "  [1.81549097]\n",
            "  [1.89657324]\n",
            "  [1.97633127]\n",
            "  [1.90686371]\n",
            "  [1.92962413]\n",
            "  [1.69363307]\n",
            "  [1.47074649]\n",
            "  [1.59216298]\n",
            "  [1.58322434]\n",
            "  [1.70069569]]\n",
            "\n",
            " [[1.60962643]\n",
            "  [1.65249878]\n",
            "  [1.8054488 ]\n",
            "  [1.86567425]\n",
            "  [2.04312275]\n",
            "  [2.05531682]\n",
            "  [1.78398503]\n",
            "  [1.78078478]\n",
            "  [1.77397045]\n",
            "  [1.86600531]\n",
            "  [1.82453996]\n",
            "  [1.89591112]\n",
            "  [1.97746239]\n",
            "  [1.9054567 ]\n",
            "  [1.92592728]\n",
            "  [1.67227966]\n",
            "  [1.47375362]\n",
            "  [1.60104644]\n",
            "  [1.58010686]\n",
            "  [1.70262688]]\n",
            "\n",
            " [[1.61020579]\n",
            "  [1.65153319]\n",
            "  [1.81165619]\n",
            "  [1.87673719]\n",
            "  [2.03956385]\n",
            "  [2.05043367]\n",
            "  [1.77303244]\n",
            "  [1.78544722]\n",
            "  [1.7750464 ]\n",
            "  [1.86895727]\n",
            "  [1.83447178]\n",
            "  [1.89522141]\n",
            "  [1.97693821]\n",
            "  [1.90454628]\n",
            "  [1.91944401]\n",
            "  [1.65492656]\n",
            "  [1.47695388]\n",
            "  [1.60794354]\n",
            "  [1.58212081]\n",
            "  [1.70022669]]\n",
            "\n",
            " [[1.61241286]\n",
            "  [1.6576854 ]\n",
            "  [1.81535303]\n",
            "  [1.8909728 ]\n",
            "  [2.03917761]\n",
            "  [2.04635059]\n",
            "  [1.76318339]\n",
            "  [1.78842676]\n",
            "  [1.77628787]\n",
            "  [1.86843309]\n",
            "  [1.84299659]\n",
            "  [1.89709742]\n",
            "  [1.97674509]\n",
            "  [1.90187021]\n",
            "  [1.91276762]\n",
            "  [1.63889771]\n",
            "  [1.48048519]\n",
            "  [1.6142061 ]\n",
            "  [1.58584524]\n",
            "  [1.69788168]]\n",
            "\n",
            " [[1.61787536]\n",
            "  [1.66245819]\n",
            "  [1.81480126]\n",
            "  [1.90644988]\n",
            "  [2.03537042]\n",
            "  [2.04240545]\n",
            "  [1.75363781]\n",
            "  [1.79350303]\n",
            "  [1.77683964]\n",
            "  [1.86890209]\n",
            "  [1.84103782]\n",
            "  [1.90873972]\n",
            "  [1.97495185]\n",
            "  [1.90129086]\n",
            "  [1.90523599]\n",
            "  [1.62480004]\n",
            "  [1.48578216]\n",
            "  [1.61464752]\n",
            "  [1.59268716]\n",
            "  [1.69523319]]\n",
            "\n",
            " [[1.6241931 ]\n",
            "  [1.66692751]\n",
            "  [1.81380808]\n",
            "  [1.92394092]\n",
            "  [2.03534283]\n",
            "  [2.03341164]\n",
            "  [1.74434052]\n",
            "  [1.79915865]\n",
            "  [1.77391527]\n",
            "  [1.8693435 ]\n",
            "  [1.84021017]\n",
            "  [1.91792665]\n",
            "  [1.97299307]\n",
            "  [1.90209092]\n",
            "  [1.89604906]\n",
            "  [1.61095067]\n",
            "  [1.48895482]\n",
            "  [1.6144544 ]\n",
            "  [1.60010844]\n",
            "  [1.69126047]]\n",
            "\n",
            " [[1.63189026]\n",
            "  [1.66891387]\n",
            "  [1.81620827]\n",
            "  [1.94264585]\n",
            "  [2.03520488]\n",
            "  [2.02516271]\n",
            "  [1.73518118]\n",
            "  [1.80009665]\n",
            "  [1.77383251]\n",
            "  [1.86332923]\n",
            "  [1.84536919]\n",
            "  [1.92388574]\n",
            "  [1.97037218]\n",
            "  [1.9045187 ]\n",
            "  [1.88677936]\n",
            "  [1.59908767]\n",
            "  [1.49243096]\n",
            "  [1.61448199]\n",
            "  [1.60794354]\n",
            "  [1.68778433]]\n",
            "\n",
            " [[1.63898047]\n",
            "  [1.6671758 ]\n",
            "  [1.8204017 ]\n",
            "  [1.95856435]\n",
            "  [2.03396341]\n",
            "  [2.01310659]\n",
            "  [1.72773231]\n",
            "  [1.80064842]\n",
            "  [1.77761212]\n",
            "  [1.85814262]\n",
            "  [1.84994887]\n",
            "  [1.92818953]\n",
            "  [1.96639945]\n",
            "  [1.90862937]\n",
            "  [1.87825455]\n",
            "  [1.58678324]\n",
            "  [1.49196196]\n",
            "  [1.61704771]\n",
            "  [1.61459234]\n",
            "  [1.68568761]]\n",
            "\n",
            " [[1.63887012]\n",
            "  [1.67509367]\n",
            "  [1.81877399]\n",
            "  [1.97536567]\n",
            "  [2.03125975]\n",
            "  [2.00254023]\n",
            "  [1.72770472]\n",
            "  [1.79891035]\n",
            "  [1.78175037]\n",
            "  [1.85154899]\n",
            "  [1.85648731]\n",
            "  [1.92943101]\n",
            "  [1.96537868]\n",
            "  [1.91006396]\n",
            "  [1.87190922]\n",
            "  [1.57599619]\n",
            "  [1.49113431]\n",
            "  [1.62022037]\n",
            "  [1.61933754]\n",
            "  [1.68908098]]\n",
            "\n",
            " [[1.63635958]\n",
            "  [1.69098458]\n",
            "  [1.81311837]\n",
            "  [1.99040134]\n",
            "  [2.02968721]\n",
            "  [1.98902193]\n",
            "  [1.73363623]\n",
            "  [1.79250985]\n",
            "  [1.78384709]\n",
            "  [1.84357595]\n",
            "  [1.86302576]\n",
            "  [1.93056213]\n",
            "  [1.96518556]\n",
            "  [1.91293315]\n",
            "  [1.86423965]\n",
            "  [1.56562295]\n",
            "  [1.48793405]\n",
            "  [1.62237226]\n",
            "  [1.6253794 ]\n",
            "  [1.69128805]]\n",
            "\n",
            " [[1.63133849]\n",
            "  [1.70229582]\n",
            "  [1.81160101]\n",
            "  [2.00212641]\n",
            "  [2.03266675]\n",
            "  [1.9722206 ]\n",
            "  [1.73862973]\n",
            "  [1.78541963]\n",
            "  [1.787875  ]\n",
            "  [1.83590638]\n",
            "  [1.86818479]\n",
            "  [1.93108631]\n",
            "  [1.96300608]\n",
            "  [1.91740247]\n",
            "  [1.85568725]\n",
            "  [1.55411859]\n",
            "  [1.48514763]\n",
            "  [1.6246621 ]\n",
            "  [1.62979354]\n",
            "  [1.69244677]]\n",
            "\n",
            " [[1.62695194]\n",
            "  [1.71504166]\n",
            "  [1.81030436]\n",
            "  [2.00965804]\n",
            "  [2.03788096]\n",
            "  [1.95900576]\n",
            "  [1.73890561]\n",
            "  [1.77973642]\n",
            "  [1.79435827]\n",
            "  [1.83008523]\n",
            "  [1.87188163]\n",
            "  [1.9344521 ]\n",
            "  [1.9612956 ]\n",
            "  [1.92203732]\n",
            "  [1.84349318]\n",
            "  [1.54553861]\n",
            "  [1.48382339]\n",
            "  [1.62722782]\n",
            "  [1.6344008 ]\n",
            "  [1.69313648]]\n",
            "\n",
            " [[1.62805547]\n",
            "  [1.72439412]\n",
            "  [1.81264937]\n",
            "  [2.01208582]\n",
            "  [2.04532982]\n",
            "  [1.9460944 ]\n",
            "  [1.74119545]\n",
            "  [1.77137714]\n",
            "  [1.80362797]\n",
            "  [1.82553314]\n",
            "  [1.87342658]\n",
            "  [1.94027325]\n",
            "  [1.95525374]\n",
            "  [1.92598246]\n",
            "  [1.83207159]\n",
            "  [1.53560679]\n",
            "  [1.48630634]\n",
            "  [1.62645535]\n",
            "  [1.64110478]\n",
            "  [1.69343995]]\n",
            "\n",
            " [[1.63056601]\n",
            "  [1.72638048]\n",
            "  [1.81824981]\n",
            "  [2.01837597]\n",
            "  [2.05148203]\n",
            "  [1.93249332]\n",
            "  [1.7462993 ]\n",
            "  [1.76699059]\n",
            "  [1.81460814]\n",
            "  [1.82095347]\n",
            "  [1.87502671]\n",
            "  [1.9460944 ]\n",
            "  [1.94739105]\n",
            "  [1.92948619]\n",
            "  [1.82114659]\n",
            "  [1.52252989]\n",
            "  [1.49430697]\n",
            "  [1.62386204]\n",
            "  [1.64678799]\n",
            "  [1.69363307]]\n",
            "\n",
            " [[1.63142125]\n",
            "  [1.73093257]\n",
            "  [1.82310537]\n",
            "  [2.02199005]\n",
            "  [2.05451676]\n",
            "  [1.91897501]\n",
            "  [1.75253427]\n",
            "  [1.76301786]\n",
            "  [1.82150524]\n",
            "  [1.81744975]\n",
            "  [1.87880632]\n",
            "  [1.94948777]\n",
            "  [1.94049395]\n",
            "  [1.93257609]\n",
            "  [1.80624886]\n",
            "  [1.51488791]\n",
            "  [1.50680451]\n",
            "  [1.61828918]\n",
            "  [1.65523003]\n",
            "  [1.692833  ]]\n",
            "\n",
            " [[1.6332145 ]\n",
            "  [1.7367813 ]\n",
            "  [1.82975417]\n",
            "  [2.02450059]\n",
            "  [2.06168974]\n",
            "  [1.90049079]\n",
            "  [1.76199709]\n",
            "  [1.7567553 ]\n",
            "  [1.83113359]\n",
            "  [1.810663  ]\n",
            "  [1.88435159]\n",
            "  [1.95221902]\n",
            "  [1.93296232]\n",
            "  [1.93751441]\n",
            "  [1.79129596]\n",
            "  [1.50512162]\n",
            "  [1.51469479]\n",
            "  [1.61473028]\n",
            "  [1.66463767]\n",
            "  [1.68874992]]\n",
            "\n",
            " [[1.63553192]\n",
            "  [1.74538888]\n",
            "  [1.83405796]\n",
            "  [2.02618348]\n",
            "  [2.06574523]\n",
            "  [1.88330323]\n",
            "  [1.76574911]\n",
            "  [1.75576211]\n",
            "  [1.84161717]\n",
            "  [1.80390385]\n",
            "  [1.88901402]\n",
            "  [1.95519856]\n",
            "  [1.92465822]\n",
            "  [1.94231479]\n",
            "  [1.77761212]\n",
            "  [1.49643128]\n",
            "  [1.52639227]\n",
            "  [1.60984714]\n",
            "  [1.671452  ]\n",
            "  [1.68640491]]\n",
            "\n",
            " [[1.6374907 ]\n",
            "  [1.75694842]\n",
            "  [1.83452696]\n",
            "  [2.02872161]\n",
            "  [2.06806265]\n",
            "  [1.86702608]\n",
            "  [1.77041155]\n",
            "  [1.75725189]\n",
            "  [1.85069375]\n",
            "  [1.7999863 ]\n",
            "  [1.89312469]\n",
            "  [1.95845399]\n",
            "  [1.91952678]\n",
            "  [1.94432874]\n",
            "  [1.76652159]\n",
            "  [1.48564422]\n",
            "  [1.53905534]\n",
            "  [1.60449499]\n",
            "  [1.67600409]\n",
            "  [1.68430819]]\n",
            "\n",
            " [[1.63864941]\n",
            "  [1.76894936]\n",
            "  [1.8332303 ]\n",
            "  [2.03236328]\n",
            "  [2.07266991]\n",
            "  [1.84956263]\n",
            "  [1.77187373]\n",
            "  [1.7610315 ]\n",
            "  [1.85441819]\n",
            "  [1.80053807]\n",
            "  [1.89386958]\n",
            "  [1.96333714]\n",
            "  [1.91538852]\n",
            "  [1.94339073]\n",
            "  [1.75402405]\n",
            "  [1.47863677]\n",
            "  [1.55215982]\n",
            "  [1.60035673]\n",
            "  [1.68058376]\n",
            "  [1.68036305]]\n",
            "\n",
            " [[1.64369809]\n",
            "  [1.77675688]\n",
            "  [1.83607191]\n",
            "  [2.03594977]\n",
            "  [2.07120773]\n",
            "  [1.83618226]\n",
            "  [1.7726738 ]\n",
            "  [1.7676803 ]\n",
            "  [1.85681838]\n",
            "  [1.80197266]\n",
            "  [1.89348334]\n",
            "  [1.96833064]\n",
            "  [1.91249174]\n",
            "  [1.94137678]\n",
            "  [1.73959532]\n",
            "  [1.47502269]\n",
            "  [1.56451942]\n",
            "  [1.59533564]\n",
            "  [1.68472202]\n",
            "  [1.67481779]]\n",
            "\n",
            " [[1.65100901]\n",
            "  [1.78621969]\n",
            "  [1.83816863]\n",
            "  [2.03796372]\n",
            "  [2.0696076 ]\n",
            "  [1.82338125]\n",
            "  [1.77212203]\n",
            "  [1.77328074]\n",
            "  [1.85745291]\n",
            "  [1.80415214]\n",
            "  [1.89522141]\n",
            "  [1.97351725]\n",
            "  [1.91166409]\n",
            "  [1.93831447]\n",
            "  [1.72469759]\n",
            "  [1.47336739]\n",
            "  [1.57315458]\n",
            "  [1.59100427]\n",
            "  [1.69043281]\n",
            "  [1.66701027]]\n",
            "\n",
            " [[1.65572662]\n",
            "  [1.79634463]\n",
            "  [1.84153441]\n",
            "  [2.0420468 ]\n",
            "  [2.06916619]\n",
            "  [1.80980776]\n",
            "  [1.77341868]\n",
            "  [1.77546022]\n",
            "  [1.86073593]\n",
            "  [1.80666269]\n",
            "  [1.89759401]\n",
            "  [1.97633127]\n",
            "  [1.90929149]\n",
            "  [1.93478316]\n",
            "  [1.7106551 ]\n",
            "  [1.47074649]\n",
            "  [1.58336228]\n",
            "  [1.58573489]\n",
            "  [1.69633673]\n",
            "  [1.65757505]]\n",
            "\n",
            " [[1.65705087]\n",
            "  [1.80222096]\n",
            "  [1.85050063]\n",
            "  [2.04312275]\n",
            "  [2.06309674]\n",
            "  [1.79491004]\n",
            "  [1.77885359]\n",
            "  [1.77397045]\n",
            "  [1.86286023]\n",
            "  [1.81549097]\n",
            "  [1.89657324]\n",
            "  [1.97746239]\n",
            "  [1.90686371]\n",
            "  [1.92962413]\n",
            "  [1.69363307]\n",
            "  [1.47375362]\n",
            "  [1.59216298]\n",
            "  [1.58322434]\n",
            "  [1.70069569]\n",
            "  [1.64882953]]\n",
            "\n",
            " [[1.65249878]\n",
            "  [1.8054488 ]\n",
            "  [1.86567425]\n",
            "  [2.03956385]\n",
            "  [2.05531682]\n",
            "  [1.78398503]\n",
            "  [1.78078478]\n",
            "  [1.7750464 ]\n",
            "  [1.86600531]\n",
            "  [1.82453996]\n",
            "  [1.89591112]\n",
            "  [1.97693821]\n",
            "  [1.9054567 ]\n",
            "  [1.92592728]\n",
            "  [1.67227966]\n",
            "  [1.47695388]\n",
            "  [1.60104644]\n",
            "  [1.58010686]\n",
            "  [1.70262688]\n",
            "  [1.64234626]]\n",
            "\n",
            " [[1.65153319]\n",
            "  [1.81165619]\n",
            "  [1.87673719]\n",
            "  [2.03917761]\n",
            "  [2.05043367]\n",
            "  [1.77303244]\n",
            "  [1.78544722]\n",
            "  [1.77628787]\n",
            "  [1.86895727]\n",
            "  [1.83447178]\n",
            "  [1.89522141]\n",
            "  [1.97674509]\n",
            "  [1.90454628]\n",
            "  [1.91944401]\n",
            "  [1.65492656]\n",
            "  [1.48048519]\n",
            "  [1.60794354]\n",
            "  [1.58212081]\n",
            "  [1.70022669]\n",
            "  [1.63972536]]], shape=(30, 20, 1), dtype=float64)\n",
            "[0.69693893 0.42862207 0.54406416 0.37137854 0.11495229 0.35167295]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}